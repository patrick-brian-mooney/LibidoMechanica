#!/LibidoMechanica/bin/python3
# -*- coding: utf-8 -*-
"""generate.py creates the content at LibidoMechanica.tumblr.com, which is a
blog consisting of automatically written "love poetry" created by this script.
This program is copyright 2017-19 by Patrick Mooney.


Usage:

    ./generate.py [options]


Options:

    --help, -h      Print this help text, then exit.
    --build, -b     Fully populate the textual similarity cache, then exit.
    --clean, -c     Clean out stale data from the textual similarity cache,
                    then exit.

Only one of the above options may be specified; if one is, the specified task
is performed, then the program quits. If no options are given on the command
line, the program writes and posts a poem, then quits.

In a nutshell, this program "writes" these "love poems" by training a Markov
chain-based text generator on a set of existing love poems (a phrase sometimes
rather broadly interpreted) picked from a larger corpus of love and romance
poetry in English. This corpus is itself a work in progress and has known
problems: as of this writing (1 July 2018), for instance, it disproportionately
emphasizes canonical British poets from the 15th to the 18th centuries and
under-represents poems that (for instance) were written in languages other than
English; were written by colonial subjects; were written by working-class
writers; etc. etc. etc. SOME attention (though not enough) has been paid to
such representational matters, and diversifying the corpus of training texts in
many ways is a long-term goal for this project.

The text generator used is a modification of my own Markov chain-based text
generator, forked from Harry R. Schwartz's text generator and heavily modified.
This particular version of the text generator treats individual characters as
the tokens that are mapped, rather than whole words, as my other text-
generation projects do. The generator is trained on a variable series of texts
selected from the corpus, each of which bears SOME computable "similarity" to
another text already selected -- either a seed text from the beginning or
another text previously selected in the same manner. The details and thresholds
for the selection algorithm are currently (25 July 2018) being tweaked quite
regularly.

Once the poem is generated by the text generator, it is modified in various
ways to make it look less like it was generated by an algorithm that was merely
traversing a series of Markov chains randomly. Some of these tasks are:

  * attempting to balance opening and closing punctuation   (#FIXME: not good enough yet.)
  * curlifying quotes
  * preventing sentence-ending punctuation from beginning a line
  * stripping spurious line breaks
  * attempting to normalize (according to any of several definitions) stanza
    length.
  * attempting to regularize the number of syllables in a line, according to
    any of several methodologies.

That was not a comprehensive list.

Once the poem is cleaned and otherwise "edited," it is posted to Tumblr, and an
archival copy is posted to the archives/ folder inside the project's folder.
This "archival copy" is a bzipped JSON file recording the poem itself, the
texts used to train the text generator that generated it, and some other info
about the poem's generation. The archives/ folder is periodically cleaned by
hand into a series of folders each containing only 1000 archived poems.

The training-text selection algorithm evaluates the "similarity" between
existing members of the list and candidates for addition to it. This is a
fairly expensive calculation to make, especially when at least one of the texts
being compared is long, and so the results of the calculation are cached
between runs in a global similarity cache. This cache is opened, used and
modified, and then updated on disk when the source text selections have been
made. There are several BasicSimilarityCache classes, though only one is actually
used by the current setup: older ones still exist in case any cache files
created by them need to be read. All are singleton classes (and more recent
ones make some attempt to enforce this, or at least to protect against
erroneous spurious creations). Creating one automatically reads the cache
into memory as an attribute of the object being instantiated. Normally, this is
probably best done with the convenience wrapper open_cache(), which is a
context manager that ensures the cache is written back to disk when it is done
being used (and has quite likely been modified). The convenience function
clean_cache() cleans stale data out of the cache; the convenience function
build_cache() forces it to be fully populated with results for all texts in the
training corpus (and takes a REALLY LONG TIME to run if the cache has not
already been populated).

This whole script is very much a rough draft and a work in progress. Many of
the sub-tasks that this script accomplishes are accomplished in hacky and
suboptimal ways. There's plenty of room for improvement here.

THIS SOFTWARE IS OFFERED WITHOUT WARRANTY OF ANY KIND AT ALL. It is ALPHA
SOFTWARE; if you don't know what that means, or can't read the source code to
determine whether it meets your needs, then this software is not intended for
you.

Nevertheless, if you want to adapt it, this script is licensed under the GNU
GPL, either version 3, or (at your option) any later version; see the file
LICENSE.md for more details.
"""

# Current list of things so annoying that they're likely to get priority in fixing:
# * Deepening and diversifying the corpus is always a goal.
# * Something is occasionally truncating poems early.           --FIXED?
#   * It seems to be connected to the lax reduce-single-lines postprocessing schema.
#   * Should start to passively diagnose problems like this by moving copies of their JSON archive data to folders
#     that track instances of those problems.
#   * comm -1 -2 a.json b.json (or such) should then help look for similarities.
# * We should syllabify the entire source corpus, keeping a list of which words are manually syllabified, then
#   check to see if they're syllabified correctly, and keep a second dictionary to use in addition to the CMU
#   corpus.
# * There are other ideas for how to judge source-text similarity:
#   * (approximate) year of composition
#   * geographical nearness
#   * various author characteristics
#     * just picking multiple poems from a single author would be a good move with authors who have a sufficient number of poems in the corpus.
#   * all of these would require manual metadata entry. Oh boy, another pickled dictionary or something.
# * Tokenizing currently drops leading space, which shouldn't happen, actually.
# * We should have CAPITALIZATION NORMALIZATION goin' on. In the output, I mean.
#   * Also other formal things: patterns of leading space, e.g.
# * When a poem title needs to be shortened, the current algorithm simply lops off a random number of tokens until the
#   phrase is short enough. This works sometimes, but also produces titles that, say, end with conjunctions an
#   unpleasant amount of the time. It would be smarter to generate a parse tree for the relevant sentence,
#   then grab an appropriate-length branch (or branches) from it.
#   * Maybe we should shorten the maximum length a bit, too.
# * There is of course always parameter tweaking. Documentation improvements, too.
# * We should try harder to avoid producing poems with a prime number of lines.
#   * Come right down to it, we should also try harder to avoid producing poems with a PRIME-LIKE number of syllables.
#     (By which I mean: no USEFUL factors in the number. It's surprising how many poems are generated with a total
#     number of syllables that has no factors that are plausible poetic line lengths.)
# * Over the long term, we need a better way to store and access the similarity cache. If we double the size of the
#   corpus from here, we would be well over 100MB in a single bzipped, already-pickle-compressed pickle file. That's
#   super-unwieldy. Plus, having all of that in memory at once already causes occasional problems at this size. I
#   need to learn some sort of simple database implementation before the corpus gets much bigger.    --WORKING
# * The directory structure needs reworking, and this module needs to be split into smaller files.   --WORKING
#   * This probably implies a utils/ folder for secondary scripts, like check_corpus.py.
#     * There will almost certainly be others.
# * There's still trouble with intra-word apostrophes.
#   * Same deal with leading apostrophes in archaic contractions with an initial dropped-letters apostrophe ("'tis")
#     * Probably best dealt with by preprocessing and using something apostrophe-like in the source texts.
# * "Is it an opening or a closing quote?" gets it wrong when:
#   * there is a previous non-alphabetic (or -alphanumeric?) character:
#     * e.g., em dash-apostrophe-capital letter should turn the apostrophe into an opening single quote, but gets it wrong


import bz2
import datetime
import functools
import glob
import json
import pprint
import random
import re
import shlex
import sys
import unicodedata

import pid                                              # https://pypi.python.org/pypi/pid/

from nltk.corpus import cmudict                         # nltk.org

import pyximport; pyximport.install()                   # http://cython.org


import patrick_logger                                   # https://github.com/patrick-brian-mooney/personal-library
from patrick_logger import log_it

import file_utils as fu                                 # https://github.com/patrick-brian-mooney/python-personal-library/
import social_media                                     # https://github.com/patrick-brian-mooney/personal-library
from social_media_auth import libidomechanica_client    # Unshared file that contains authentication tokens.


import poetry_generator as pg                           # https://github.com/patrick-brian-mooney/markov-sentence-generator
import text_handling as th                              # https://github.com/patrick-brian-mooney/personal-library

from globs import *                                 # Filesystem structure, etc.
import cython_experiments.similarity_cache.similarity_cache as sc      # Cache of calculated textual similarities.


patrick_logger.verbosity_level = 3

syllable_dict = cmudict.dict()

# This next is a global dictionary holding data to be archived at the end of the run. Modified constantly.
post_data = {'tags': ['poetry', 'automatically generated text', 'Patrick Mooney', 'Markov chains'],
             'normalization_strategy': None,
             'syllabic_normalization_strategy': None,
             'stanza length': None,
             }

genny = None            # We'll reassign this soon. We want it to be defined in the global namespace early, though.


def print_usage(exit_code=0):
    """Print the docstring as a usage message to stdout, then quit with status code
    EXIT_CODE.
    """
    log_it("INFO: print_usage() was called", 4)
    print(__doc__)
    sys.exit(exit_code)


def factors(n):
    """Return a list of the factors of a number. Based on code at
    < https://stackoverflow.com/a/6800214 >.
    """
    assert (int(n) == n and n > 1), "ERROR: factors() called on %s, which is not a positive integer" % n
    return sorted(list(set(functools.reduce(list.__add__, ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))))


def manually_count_syllables(word):
    """Clearly not perfect, but better than nothing.

    #FIXME: we should be keeping an additional list for words not in cmudict.

    Based on https://datascience.stackexchange.com/a/24865.
    """
    count = 0
    vowels = 'aeæiouy'
    word = unicodedata.normalize('NFKD', word.lower()).encode('ASCII', 'ignore').decode('ASCII')    # strip diacritics
    if len(word) == 0: return 0             # Naively assume that null words have no syllables.
    if word[0] in vowels:
        count +=1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index-1] not in vowels:
            count +=1
    if word.endswith('e'):
        count -= 1
    if word.endswith('le'):
        count += 1
    if count == 0:
        count += 1
    return count


def syllable_count(word):
    """Do a reasonably good job of determining the number of syllables in WORD, a word
    in English. Uses the CMU corpus if it contains the word, or a best-guess
    approach otherwise. Based on https://stackoverflow.com/a/4103234.
    """
    w = ''.join([c for c in word if c.isalpha()])
    try:
        return sum([len(list(y for y in x if y[-1].isdigit())) for x in syllable_dict[w.lower()]])
    except KeyError as err:
        log_it("Word %s is apparently not in CMUDict: %s" % (shlex.quote(word), err), 5)
        return manually_count_syllables(w)


def is_prime(n):
    """Return True if N is prime, false otherwise. "Prime" is here defined specifically
    as "has fewer than three factors," which is not quite the same as mathematical
    ... um, primery. Primeness. Anyway, this is intended to be inclusive about edge
    cases that "is it prime?" should often include (at least for the purposes of
    this particular project) rather than be an exact mathematically correct test.
    """
    assert (int(n) == n and n >= 1), "ERROR: is_prime() called on %s, which is not a positive integer" % n
    return (len(factors(n)) < 3)


def lines_without_stanza_breaks(the_poem):
    """Returns a *list* of lines from THE_POEM, ignoring any blank lines."""
    return [l for l in the_poem.split('\n') if len(l.strip()) > 0]


def total_lines(the_poem):
    """Returns the total number of non-empty lines in the poem."""
    ret = len(lines_without_stanza_breaks(the_poem))
    log_it("    current total lines in poem: %d" % ret, 5)
    return ret


def remove_single_lines(the_poem, combination_probability=0.85):
    """Takes the poem passed as THE_POEM and goes through it, (randomly) eliminating
    single-line stanzas. Returns the modified poem.
    """
    log_it("Removing single lines from the poem with probability %f %%" % (100 * combination_probability), 3)

    # First, produce a list of stanzas, each of which is a list of lines
    stanzas = [[l for l in s.split('\n') if l.strip()] for s in the_poem.split('\n\n')]
    i = 0
    while (i + 1) < len(stanzas):
        if len(stanzas[i]) < 3:                 # If the length of this stanza is less than three ...
            try:                                # Combine it with the next stanza. Probably.
                if random.random() <= combination_probability:
                    next_stanza = stanzas.pop(i+1)
                    stanzas[i] += next_stanza
                else: i += 1
            except IndexError:                  # If there is no next stanza ...
                if len(stanzas) > 1:            # ... add this stanza to the end of the previous stanza.
                    stanzas[-1] += stanzas.pop()
        else:
            i += 1
    return '\n\n'.join(['\n'.join(s) for s in stanzas])


def regularize_form(the_poem):
    """Tries to find a form that gives THE_POEM a more or less regular syllables-per-
    line pattern. This is another series of rough approximations, of course.

    As it tries various strategies, it attempts to build a FORM list, which is
    simply a list of syllable counts, line by line: so, [10, 10, 10, 10] describes
    a poem with four ten-syllable lines. These numbers are not firm plans, but
    rather a series of rolling goals: what the sample list just given actually
    means, more specifically, is: break off a line with ten or more syllables. Then
    break off a second line that brings the total syllables in the poem to at least
    twenty. Then another line that brings the total syllables to at least thirty.
    Then another line that brings the total syllables to at least forty.

    Once it has made basic choices, it may then go back through and vary the plans,
    using any of several traditional forms as a rough model for syllabic variation.
    It keeps track of any syllables over the model it has constructed has relative
    to the number of syllables in the source text that is going to be reformatted;
    at the end of the planning process, it randomly removes syllables from anywhere
    at all in the plan in order to make the number fit. As of this writing (25 July
    2018), the total syllabic debt should never exceed one, but this may change in
    the future.

    Once the plans are settled, the poem is made to conform as closely as possible
    to the expected plan without breaking words between lines.

    Once that is done, this function then re-groups the lines according to any of
    several strategies, emphasizing (when possible) regular stanza lengths. More
    work is needed to integrate this more tightly with syllabic normalization.

    Once all of this is done, the function returns the modified poem.

    #FIXME: Currently, the algorithm chops off chunks into lines as soon as it
    reaches or exceeds its syllabic goal. However, this has two downsides:
        1. Some zero-syllable-length tokens (e.g., much punctuation) would be
           better placed at the end of the line than at the beginning of the next
           line, but this doesn't happen because we stop as soon as we meet our
           syllabic goal. (We should be looking ahead at the next token.) This is
           why, for instance, some poems generated recently have commas at the
           beginnings of lines.

           * This is definitely not the case with all zero-length punctuation,
             though: the quotation dash should bind right more strongly than it
             binds left, for instance.

        2. We should be considering how far past the current goal the current token
           takes us before deciding whether to incorporate it into the current line,
           rather than just taking whatever's next, no matter how far past the goal
           this puts us. Currently, if we're (say) one syllable short of our current
           syllabic goal, we take a token no matter what the token is; it might
           happen to put us six syllables past our current goal if it's a very long
           word. It would be smarter to scan the token and count syllables before
           making the decision, then base than decision on which choice leaves us
           closer to our current goal at the end of the line.
    """
    global post_data
    log_it("Attempting to regularize poetic form ...", 3)
    form = None
    syllable_debt = 0
    total_syllables = sum([syllable_count(word) for word in genny._token_list(the_poem, character_tokens=False)])
    syllabic_factors = factors(total_syllables)

    # First_attempt: is any of the factors a plausible line length?
    single_line_counts = list(set(syllabic_factors) & set(range(7, 17)))
    if single_line_counts and random.random() < 0.8:
        length = random.choice(single_line_counts)
        form = [length] * (total_syllables // length)
        post_data['syllabic_normalization_strategy'] = 'Regular line length: %d syllables' % length
        log_it("    ... basic strategy: %d syllables per line" % length, 4)
    elif random.random() < 0.4:
        length = total_syllables / the_poem.count('\n')
        form = [length] * the_poem.count('\n')
        post_data['syllabic_normalization_strategy'] = 'Fractional regular line length, based on average: %.4g syllables' % length
        log_it("    ... basic strategy: %f syllables per line" % length, 4)
    elif random.random() < 0.7:
        while not form:
            syllable_debt += 1      # There's a good chance we've already tried, and failed, with zero.
            syllabic_factors = factors(syllable_debt + total_syllables)
            single_line_counts = sorted(list(set(syllabic_factors) & set(range(7, 17))))
            if single_line_counts:
                length = random.choice(single_line_counts)
                form = [length] * ((total_syllables + syllable_debt) // length)
                post_data['syllabic_normalization_strategy'] = 'Regular line length: %d syllables, with syllabic debt of %d' % (length, syllable_debt)
        log_it("    ... basic strategy: %d syllables per line, with debt of %d syllables" % (length, syllable_debt), 4)

    # First, vary the form planned, as appropriate. Once again, much more work needed (or at least possible) here.
    log_it("INFO: Deciding whether to apply transformations to basic pattern ...", 3)
    if form:
        count = 0
        if (len(form) % 9 == 0) and (min(form) >= 6) and (random.random() < 0.4):
            log_it("    choosing richatameter as basic modification model", 4)
            post_data['stanza length'] = 9          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (richtameter-like)"
            while count < len(form):
                form[count]   -= (32/9)
                form[count+1] -= (14/9)
                form[count+2] += (4/9)
                form[count+3] += (22/9)
                form[count+4] += (40/9)
                form[count+5] += (22/9)
                form[count+6] += (4/9)
                form[count+7] -= (14/9)
                form[count+8] -= (32/9)
                count += 9
        elif (len(form) % 6 == 0) and (random.random() < 0.5):
            log_it("    choosing Burns stanza as basic modification model", 4)
            if random.random() < 0.8:
                post_data['stanza length'] = 6          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (6-line Burns stanza-like)"
            while count < len(form):
                form[count]   += (4/3)
                form[count+1] += (4/3)
                form[count+2] += (4/3)
                form[count+3] -= (8/3)
                form[count+4] += (4/3)
                form[count+5] -= (8/3)
                count += 6
        elif (len(form) % 5 == 0) and (random.random() < 0.5):
            log_it('    choosing five-line "ballad" as basic modification model', 4)
            if random.random() < 0.7:
                post_data['stanza length'] = 5          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += ' (5-line alternating "ballad" with initial debt of %d)' % syllable_debt
            while count < len(form):
                sign = 1 if syllable_debt <= 0 else -1
                form[count]   += (1 * sign)
                form[count+1] -= (1 * sign)
                form[count+2] += (1 * sign)
                form[count+3] -= (1 * sign)
                form[count+4] += (1 * sign)
                count += 5
                syllable_debt += sign
        elif (len(form) % 5 == 0) and (random.random() < 0.25):
            log_it("    choosing cinquain as basic modification model", 4)
            if random.random() < 0.7:
                post_data['stanza length'] = 5          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (cinquain-like)"
            while count < len(form):
                form[count]   -= 2.4
                form[count+1] -= 0.4
                form[count+2] += 1.6
                form[count+3] += 3.6
                form[count+4] -= 2.4
                count += 5
        elif (len(form) % 4 == 0) and (random.random() < 0.4):
            log_it('    choosing "ballad" with short last line with as basic modification model', 4)
            if random.random() < 0.6:
                post_data['stanza length'] = 4          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (4-line ballad-like with short last line)"
            while count < len(form):
                form[count] += 1
                form[count + 2] += 1
                form[count + 3] -= 2
                count += 4
        elif (len(form) % 3 == 0 and (random.random() < 0.333333)):
            log_it("    choosing haiku as basic modification model", 4)
            if random.random() < 0.4:
                post_data['stanza length'] = 3          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (3-line haiku-like)"
            while count < len(form):
                form[count] -= 1
                form[count + 1] += 2
                form[count + 2] -= 1
                count += 3
        elif (len(form) % 3 == 0 and (random.random() < 0.333333)):
            log_it("    choosing short-middle-line terza rima as basic modification model", 4)
            post_data['stanza length'] = 3          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (3-line terza rima-like with short middle line)"
            while count < len(form):
                form[count] += 1
                form[count + 1] -= 2
                form[count + 2] += 1
                count += 3
        elif (len(form) % 2 == 0) and (random.random() < 0.6):
            log_it("    choosing two-line ballad-like stanza as basic modification model", 4)
            if random.random() < 0.3:
                post_data['stanza length'] = 2          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (2-line ballad-like)"
            while count < len(form):
                form[count] += 1
                form[count+1] -= 1
                count += 2

        # Next, pay off any syllabic debt.
        if syllable_debt:
            log_it("... paying off syllabic debt", 3)
            for i in random.sample(range(len(form)), syllable_debt):    # Choose random lines from the poem
                log_it("    ... removing one syllable from line %d" % i, 5)
                form[i] -= 1                                            # ... they can pay off the debt.
        post_data['form_plan'] = '[' + ', '.join('%.5g' % i for i in form) + ']'

        # OK, now make the poem conform (approximately) to the form we planned.
        log_it("INFO: re-breaking poetic lines ...", 4)
        tokenized_poem = genny._token_list(the_poem, character_tokens=False)
        working_copy = ''.join(list(the_poem))
        lines, total_syllables = [][:], 0
        current_line = ''
        current_goal = form.pop(0)
        while tokenized_poem:
            current_token = tokenized_poem.pop(0)
            try:
                next_token = tokenized_poem[0]              # Look, but don't pop it off the stack
                current_token_with_context = working_copy[:working_copy.find(next_token)]
            except IndexError:                              # At end of poem? "token with context" *is* "token", then.
                current_token_with_context = working_copy[:]
                next_token = ""
            original_len = len(current_token_with_context)      # We're about to modify current_token_with_context, but we'll need to operate on working_copy based on original length
            if current_token_with_context.count('\n'):
                current_token_with_context = th.multi_replace(current_token_with_context, [['\n', ' '], ['  ', ' ']])
            working_copy = working_copy[original_len:]
            current_line += current_token_with_context
            total_syllables += syllable_count(current_token)
            if total_syllables >= current_goal:         # We've (probably) hit a line break. Reset the things that need to be reset.
                # If we're out of poem, or we're at/past our syllabic target and the next token is not zero-length, finalize the line and reset.
                if (not tokenized_poem) or (syllable_count(next_token) != 0):
                    lines += [current_line + '\n' if not current_line.endswith('\n') else ""]
                    current_line = ''
                    try:
                        current_goal += form.pop(0)
                    except IndexError:                  # No more lines left? Just run to end of poem.
                        current_goal += sum([syllable_count(word) for word in tokenized_poem])
        if tokenized_poem:
            raise RuntimeError("There is leftover material that has not been put into the new poem!")
        the_poem = ''.join(lines)

    # OK, now that we've rearranged words from one line to another, we modify the overall stanza form of the poem.
    textual_lines = lines_without_stanza_breaks(the_poem)
    post_data['normalization_strategy'] = None
    if post_data['stanza length'] or ((not is_prime(len(textual_lines))) and (random.random() < 0.8)):
        if not post_data['stanza length']:
            post_data['normalization_strategy'] = 'regular stanza length'
            possible_stanza_lengths = factors(len(textual_lines))
            if len([x for x in possible_stanza_lengths if x >= 3]):     # If possible, prefer stanzas at least as long as Dante's in the Divine Comedy.
                possible_stanza_lengths = [x for x in possible_stanza_lengths if x >= 3]
            if len([x for x in possible_stanza_lengths if x <= 16]):    # If possible, choose a stanza length no longer than Meredith's extended sonnets in /Modern Love/.
                possible_stanza_lengths = [x for x in possible_stanza_lengths if x <= 16]
            post_data['stanza length'] = random.choice(possible_stanza_lengths)
            if post_data['stanza length'] == len(textual_lines):
                pass
            if post_data['stanza length'] == 1:
                post_data['stanza length'] = len(textual_lines)      # 1 long stanza, not many one-line stanzas
        the_poem = ""
        for stanza in range(0, len(textual_lines) // post_data['stanza length']):  # Iterate over the appropriate # of stanzas
            for line in range(0, post_data['stanza length']):
                the_poem += "%s\n" % textual_lines.pop(0)
            the_poem += '\n'  # Add stanza break
    elif (not the_poem.count('\n\n')) and (random.random()) < 0.85:
        post_data['normalization_strategy'] = 're-introduce random stanza breaks'
        poem_lines = the_poem.split('\n')
        for i in range(0, 1 + random.randint(1, len(textual_lines) // 3)):
            poem_lines[random.randint(0, len(poem_lines) - 1)] += '\n'
        the_poem = '\n'.join(poem_lines)
    elif random.random() < 0.8:
        post_data['normalization_strategy'] = 'remove single lines (strict)'
        the_poem = remove_single_lines(the_poem, combination_probability=1)
#    if (post_data['normalization_strategy'] == 're-introduce random stanza breaks') or (post_data['normalization_strategy'] == None and (random.random() < 0.8)):
#        if post_data['normalization_strategy'] == 're-introduce random stanza breaks':
#            post_data['normalization_strategy'] += ' + remove single lines (lax)'
#        else:
#            post_data['normalization_strategy'] = 'remove single lines (lax)'
#        the_poem = remove_single_lines(the_poem, combination_probability=0.9)
    return the_poem


def count_previous_untitled_poems():
    ret = len([x for x in fu.get_files_list(post_archives, None) if 'untitled' in x.lower()])
    log_it("Counted previous untitled poems: %d" % ret, 4)
    return ret


def balance_punctuation(the_poem, opening_char, closing_char):
    """Makes sure that paired punctuation (smart quotes, parentheses, brackets) in the
    poem are 'balanced.' If not, it attempts to correct it.
    """
    opening, closing = the_poem.count(opening_char), the_poem.count(closing_char)
    if closing_char == '’':     # Sigh. We have to worry about apostrophes that look like closing single quotes.
        closing -= len(re.findall('[:alnum:]*’[:alnum:]', the_poem))    # Inside a word? It's an apostrophe. Don't count it.

    log_it("INFO: Balancing %s and %s (%d/%d)" % (opening_char, closing_char, opening, closing), 2)

    if opening or closing:      # Do nothing if there's no instances of either character
        if opening != closing:  # Do nothing if we already have equal numbers (even if not properly "balanced")
            nesting_level = 0   # How many levels deep are we right now in the punctuation we're tracking?
            indexed_poem = list(the_poem)
            index = 0
            while index <= (len(indexed_poem)-1):
                char = indexed_poem[index]
                next_char = '' if index == len(indexed_poem) -1 else indexed_poem[index + 1]
                last_char = '' if index == 0 else indexed_poem[index - 1]
                if index == (len(indexed_poem)-1) :  # End of the poem?
                    if nesting_level > 0:               # Close any open characters.
                        indexed_poem += [closing_char]
                        nesting_level -= 1
                    index += 1
                elif char == opening_char:          # Opening character?
                    if index == len(indexed_poem):      # Last character is an opening character?
                        indexed_poem.pop(-1)            # Just drop it.
                    else:
                        nesting_level += 1              # We're one level deeper
                        index += 1                          # Move on to next character
                elif char == closing_char:          # Closing character?
                    if (closing_char == '’') and (th.is_alphanumeric(next_char) and th.is_alphanumeric(last_char)):
                        index += 1      # Skip apostrophes in the middle of words
                    else:
                        if nesting_level < 1:               # Are we trying to close something that's not open?
                            indexed_poem.pop(index)         # Just drop the spurious close quote
                        else:
                            if next_char.isspace():             # Avoid non-quote apostrophes in middle of words.
                                nesting_level -= 1          # We're one level less deep
                            index += 1                      # Move on to next character
                elif nesting_level > 0:             # Are we currently in the middle of a bracketed block?
                    if next_char.isspace():             # Is the next character whitespace?
                        if random.random() < (0.001 * nesting_level):   # Low chance of closing the open bracket
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    elif char in ['.', '?', '!'] and next_char.isspace():
                        if random.random() < (0.05 * nesting_level):    # Higher chance of closing the open bracketer
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                            if random.random() < 0.2:                       # Force new paragraph break?
                                indexed_poem.insert(index + 1, '\n')
                    elif char in known_punctuation and last_char in ['.', '!', '?']:
                        if random.random() < (0.05 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                            if random.random() < 0.2:                       # Force new paragraph break?
                                indexed_poem.insert(index + 1, '\n')
                    elif char == '\n' and next_char == '\n':            # Very high chance of closing on paragraph boundaries
                        if random.random() < (0.4 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    elif char == '\n':
                        if random.random() < (0.1 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    index += 1
                else:
                    index += 1
            the_poem = ''.join(indexed_poem)
    log_it("   ... after balancing, there are %d/%d punctuation marks." % (the_poem.count(opening_char), the_poem.count(closing_char)), 3)
    return the_poem


def HTMLify(the_poem):
    """Return a version of THE_POEM that ends lines with <br /> and wraps stanzas
    in <p> ... </p>.
    """
    # Add HTML <br /> to end of every line
    ret = '\n'.join([line.rstrip() + '<br />' for line in the_poem.split('\n')])
    # Wrap stanzas in <p> ... </p>
    ret = '\n'.join(['<p>%s</p>' % line for line in ret.split('<br />\n<br />')])
    # Eliminate extra line breaks at the very beginning of paragraphs
    ret = th.multi_replace(ret, [['<p><br />\n', '<p>'], ['<p>\n', '<p>'], ['<p>\n', '<p>']])
    return ret


def appropriate_quote(quote_list, quote_level, standard_english_quotes=False):
    """Returns the appropriate quotation mark from QUOTE_LIST, which is a list of
    (opening or closing) quotation marks. The quotation mark is chosen to be the
    correct mark to open or close a quote at QUOTE_LEVEL levels of nesting. If
    STANDARD_ENGLISH_QUOTES is False (the default), then American-style rules for
    quotes are used (double, then single, then double, then single, then double,
    then single ...); otherwise, Standard English rules (single, double, single ...)
    are used.

    This function makes a number of assumptions about the structure of QUOTE_LIST:
        * it must contain at least two strings.
        * the first string must be a single quote.
        * the second string must be a double quote.
        * any other strings in the list are ignored and not used at all by this
          function, though they may be useful to other bits of code in this
          project.

    Here's a quick summary of the first few levels of nesting, for reference, and
    assuming that the lists passed are the global constants open_quotes and
    close_quotes:

              Standard English            American English
    level   opening     closing         opening     closing
    -----   -------     -------         -------     -------
      1       ‘            ’               “           ”
      2       “            ”               ‘           ’
      3       ‘            ’               “           ”
      4       “            ”               ‘           ’
      5       ‘            ’               “           ”
                                 etc.
    """
    return quote_list[((0 if standard_english_quotes else 1) + quote_level - 1) % 2]


def normalize_quotes(the_poem, standard_english_quotes=False):
    """Takes THE_POEM, a string representing an entire poem, and makes sure that
    quotation marks are used "correctly": not only are there the same number of
    opening and closing quotation marks, but they are properly nested, with single
    and double quotes alternating properly, and opening quotes always preceding
    closing quotes in an appropriate fashion.

    As with much of the other quote-handling code in this project, assumes that
    quotation marks are single characters.

    Returns a string, which is the entire modified poem.

    #FIXME: currently, STANDARD_ENGLISH_QUOTES is *never* True. Should it be?
    """
    ret = ""
    quote_depth = 0
    for i, c in enumerate(the_poem):
        if c in open_quotes:
            if i < (len(the_poem)-1):       # Special-case 'tis, 'twas, 'gainst, etc.
                context = the_poem[i+1:]
                for which_exception in words_with_initial_apostrophes:
                    if context.startswith(which_exception) and context[len(which_exception)].isspace():
                        ret += close_quotes[0]
                        continue
            quote_depth += 1
            ret += appropriate_quote(open_quotes, quote_depth, standard_english_quotes=standard_english_quotes)
            continue
        if c in close_quotes:
            if quote_depth < 1:     # is there no current quote to close? move along, dropping this quotation mark
                continue
            if i == 0:              # don't open the poem with a closing quote mark. Just move on.
                continue
            if i < (len(the_poem) - 1): # If we've got at least one more character in the poem, make sure this closing quote isn't actually an apostrophe.
                if th._is_alphanumeric_char(the_poem[i-1]) and th._is_alphanumeric_char(the_poem[i+1]):
                    continue
            ret += appropriate_quote(close_quotes, quote_depth, standard_english_quotes=standard_english_quotes)
            quote_depth -= 1
            continue
        else:
            ret += c
    # before finishing, close any remaining open quotations.
    while quote_depth > 0:
        ret += appropriate_quote(close_quotes, quote_depth)
        quote_depth -= 1
    return ret


def fix_punctuation(the_poem):
    """Cleans up the punctuation in the poem so that it appears to be more
    'correct.' Since characters are generated randomly based on a frequency
    analysis of which characters are likely to follow the last three to ten
    characters, there's no guarantee that (for instance) parentheses or quotes
    are balanced, because the generator doesn't pay attention to or understand
    larger-scale structures.

    THE_POEM is a string, which is the text of the entire poem; the function
    returns a new, punctuation-fixed version of the poem passed in.

    NOT YET FULLY IMPLEMENTED.
    #FIXME: balancing punctuation needs to stop using heuristics and accurately
            check structure.
    #FIXME: we need to deal with the single/double quotes nesting problem.
    """
    log_it("INFO: about to alter punctuation", 2)
    the_poem = strip_invalid_chars(the_poem)
    the_poem = curlify_quotes(the_poem, "'", "‘", "’")
    the_poem = curlify_quotes(the_poem, '"', '“', '”')
    the_poem = balance_punctuation(the_poem, "‘", "’")
    the_poem = balance_punctuation(the_poem,  '“', '”')
    the_poem = balance_punctuation(the_poem,  '(', ')')
    the_poem = normalize_quotes(the_poem)
    the_poem = balance_punctuation(the_poem,  '[', ']')
    return balance_punctuation(the_poem,  '{', '}')

def get_title(the_poem):
    """Get a title for the poem. There are several title-generating algorithms; this
    function picks one at random.
    """
    log_it("INFO: getting a title for the poem", 2)
    if random.random() < (1/15):
        title = "Untitled Poem # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (1/14):
        title = "Untitled Composition # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (1/13):
        title = "Untitled # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (2/12):
        title = "Untitled (‘%s’)" % th.strip_leading_and_trailing_punctuation(the_poem.split('\n')[0]).strip()
    elif random.random() < (4/10):          # First line, in quotes
        title = th.strip_leading_and_trailing_punctuation(the_poem.strip().split('\n')[0]).strip()
    elif random.random() < (3/6):           # Pick one of the first three lines
        title = "‘%s’" % th.strip_leading_and_trailing_punctuation(the_poem.split('\n')[random.randint(1,4)-1]).strip()
    else:                                   # New 'sentence' from same poem
        title = th.strip_leading_and_trailing_punctuation(genny.gen_text(sentences_desired=1).split('\n')[0].strip())
    if len(title) < 5:                              # Try again, recursively.
        title = get_title(the_poem)
    while len(title) > 70:
        words = title.split()
        title = ' '.join(words[:random.randint(3, min(12, len(words)))])
    title = title.strip()
    if title.startswith('‘') and not title.endswith('’'):       # The shortening procedure above might have stripped the closing quote
        title = title + '’'
    if '(‘' in title and not '’)' in title:
        title = title + '’)'
    title = fix_punctuation(title)
    log_it("Title is: %s" % title)
    return title


def strip_invalid_chars(the_poem):
    """Some characters appear in the training texts but are characters that, I am
    declaring by fiat, should not make it into the final generated poems at all.
    The underscore is a good example of characters in this class. This function
    takes an entire poem as input (THE_POEM) and returns a poem entirely
    stripped of all such characters.
    """
    log_it("INFO: stripping invalid characters from the poem", 2)
    invalids = ['_', '*']
    return ''.join([s for s in the_poem if not s in invalids])

def curlify_quotes(the_poem, straight_quote, opening_quote, closing_quote):
    """Goes through THE_POEM (a string) and looks for instances of STRAIGHT_QUOTE
    (a single-character string). When it finds these instances, it substitutes
    OPENING_QUOTE or CLOSING_QUOTE for them, trying to make good decisions about
    which of those substitutions is appropriate.

    IMPORTANT CAVEAT: this routine iterates over THE_POEM, making in-place
    changes at locations determined via an initial scan. This means that
    OPENING_QUOTE and CLOSING_QUOTE **absolutely must** have the same len() as
    STRAIGHT_QUOTE, or else weird things will happen. This should not be a
    problem with standard English quotes under Python 3.X; but it may fail under
    non-Roman scripts, in odd edge cases, or if the function is used to try to
    do something other than curlify quotes.

    NOT FULLY TESTED, but I'm going to bed.
    """
    log_it("INFO: curlify_quotes() called to differentiate %s (%d) into %s and %s" % (straight_quote, the_poem.count(straight_quote), opening_quote, closing_quote), 2)
    assert len(straight_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    assert len(opening_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    assert len(closing_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    index = 0
    while index < len(the_poem):
        index = the_poem.find(straight_quote, index)
        if index == -1:
            break       # We're done.
        if index == 0:                                                      # Is it the first character of the poem?
            the_poem = opening_quote + the_poem[1:]
        elif index == len(the_poem):                                        # Is it the last character of the poem?
            the_poem = the_poem[:-1] + closing_quote
        elif the_poem[index-1].isspace() and the_poem[index+1].isspace():   # Whitespace on both sides? Replace quote with space.
            the_poem = the_poem[:index] + ' ' + the_poem[1+index:]
        elif not the_poem[index-1].isspace():                               # Non-whitespace immediately before quote? It's a closing quote.
            the_poem = the_poem[:index] + closing_quote + the_poem[index+1:]
        elif not the_poem[index+1].isspace():                               # Non-whitespace just after quote? It's an opening quote.
            the_poem = the_poem[:index] + opening_quote + the_poem[index+1:]
        else:                                                               # Quote appears in middle of non-whitespace text ...
            if straight_quote == '"':
                the_poem = the_poem[:index-1] + the_poem[index+1:]                  # Just strip it out.
            elif straight_quote == "'":
                the_poem = the_poem[:index-1] + closing_quote + the_poem[index+1:]  # Make it an apostrophe.
            else:
                raise NotImplementedError("We don't know how to deal with this quote: %s " % straight_quote)
    return the_poem

def do_basic_cleaning(the_poem):
    """Does first-pass elementary cleanup tasks on THE_POEM. Returns the cleaned
    version of THE_POEM.
    """
    log_it("INFO: about to do basic pre-cleaning of poem", 2)
    the_poem = th.multi_replace(the_poem, [[' \n', '\n'], ['\n\?', '?'], ['\n!', '!'],
                                           ['\n"', '\n'], ['\n”', '\n'], ['\n\n\n', '\n\n'],
                                           ['\n" ', '\n"'], ['^" ', '"']]).rstrip()
    return the_poem


def do_final_cleaning(the_poem):
    log_it("INFO: about to do final cleaning of poem", 2)
    the_poem = th.multi_replace(the_poem, [[' \n', '\n'],               # Eliminate any spurious end-of-line spaces
                                           ['\n\n\n', '\n\n'],          # ... and any extra line breaks.
                                           [r'\n\)', r')'],             # And line breaks right before ending punctuation
                                           ['\n\?', '?'], ['\n!', '!'],
                                           ['\n"', '\n'], ['\n”', '\n'], ['\n’', '’'],
                                           ['“\n', '“'], ['"\n', '"'],  # And line breaks right after beginning punctuation
                                           ['\n—\n', '—\n'],            # Don't allow an em dash to be the only character on a line.
                                           ['`', '’']                   # Replace backticks with faux apostrophes
                                          ])
    poem_lines = the_poem.split('\n')
    index = 0
    while index < (len(poem_lines) - 1):                                # Go through, line by line, making any final changes.
        line = poem_lines[index]
        if '  ' in line.strip():                                        # Multiple whitespace in line? Break into multiple lines
            individual_lines = ['  ' + i + '\n' for i in line.split('  ')]  #FIXME: count how many spaces we're breaking on. Ugh.
            if len(individual_lines) > 1:
                poem_lines.pop(index)
                individual_lines.reverse()          # Go through the sub-lines backwards,
                for l in individual_lines:
                    index += 1
                    poem_lines.insert(index, l)     # ... inserting lines and pushing the line stack up.
        else:
            index += 1
    the_poem = '\n'.join(poem_lines)

    # OK, now let's check to make sure we don't have a duplicate last stanza.
    stanzas= the_poem.split('\n\n')
    if len(stanzas) >= 2:
        while stanzas[-1].strip().lower() == stanzas[-2].strip().lower():
            stanzas = stanzas[:-1]
    return regularize_form(the_poem)


def main():
    global genny, post_data

    if not sc.oldmethod:
        with sc.open_cache() as similarity_cache:
            sample_texts = sc.get_source_texts(similarity_cache)
    else:
        sample_texts = sc.get_source_texts(None)
    log_it(" ... selected %d texts" % len(sample_texts), 2)

    # Next, set up the basic parameters for the run
    chain_length = round(min(max(random.normalvariate(6, 0.8), 3), 10))

    # And track sources of this particular poem
    source_texts = [ th.remove_prefix(t, "Link to ").strip() for t in sample_texts ]
    post_data['tags'] += ['Markov chain length: %d' % chain_length, '%d texts' % len(sample_texts) ]

    poem_length = round(min(max(random.normalvariate(10, 5), 4), 200))          # in SENTENCES. Not lines.

    log_it("INFO: about to set up and train text generator ...")
    genny = pg.PoemGenerator(name='Libido Mechanica poetry generator', training_texts=sample_texts, markov_length=chain_length)
    log_it(" ...trained!")

    log_it("INFO: about to generate poem ...")
    the_poem = genny.gen_text(sentences_desired=poem_length, paragraph_break_probability=0.2)

    post_data['title'] = get_title(the_poem)

    log_it("poem generated; title is: %s" % post_data['title'])
    log_it("lines are: \n\n" + the_poem)
    log_it("tags are: %s" % post_data['tags'])

    log_it("INFO: cleaning poem up ...")
    the_poem = do_basic_cleaning(the_poem)
    the_poem = fix_punctuation(the_poem)
    the_poem = do_final_cleaning(the_poem)

    log_it("INFO: HTML-izing poem ...")
    # Force all spaces to be non-breaking spaces
    formatted_poem = th.multi_replace(the_poem, [[' ', '&nbsp;']])
    formatted_poem = HTMLify(formatted_poem)
    # Pretty-print (for debugging only; doesn't matter for Tumblr upload, but neither does it cause problems)
    formatted_poem = th.multi_replace(formatted_poem, [['<p>\n', '\n<p>']])
    # Prevent all spaces from collapsing; get rid of spurious paragraphs
    formatted_poem = th.multi_replace(formatted_poem, [['<p></p>', ''], ['<p>\n</p>', '']])
    log_it('INFO: Attempting to post the content...')
    post_data['status_code'], post_data['tumblr_data'] = social_media.tumblr_text_post(libidomechanica_client, ', '.join(post_data['tags']), post_data['title'], formatted_poem)
    log_it('INFO: the_status is: ' + pprint.pformat(post_data['status_code']), 2)
    log_it('INFO: the_tumblr_data is: ' + pprint.pformat(post_data['tumblr_data']), 3)

    log_it("INFO: archiving poem and metadata ...")
    post_data['text'], post_data['time'] = the_poem, datetime.datetime.now().isoformat()
    post_data['the_poem'], post_data['formatted_text'] = the_poem, formatted_poem
    post_data['sources'] = sorted(source_texts)
    archive_name = "%s — %s.json.bz2" % (post_data['time'], post_data['title'])
    last_folder = sorted([d for d in glob.glob(os.path.join(post_archives, '*')) if os.path.isdir(d)])[-1]
    if len([f for f in glob.glob(os.path.join(last_folder, "*json.bz2")) if os.path.isfile(f)]) > 999:
        new_folder = os.path.join(post_archives, "%03d" % (1 + int(os.path.basename(last_folder))))
        os.mkdir(new_folder)
        last_folder = new_folder
        with sc.open_cache() as similarity_cache:              # Every thousand poems, clean out the cache.
            similarity_cache.clean_cache()
    with bz2.BZ2File(os.path.join(last_folder, archive_name), mode='wb') as archive_file:
        archive_file.write(json.dumps(post_data, sort_keys=True, indent=3, ensure_ascii=False).encode())
    log_it("INFO: We're done")



if __name__ == "__main__":
    if len(sys.argv) == 2:
        if sys.argv[1] in ['--help', '-h']:
            print_usage()
        elif sys.argv[1] in ['--clean', '-c']:
            sc.clean_cache()
        elif sys.argv[1] in ['--build', '-b']:
            sc.build_cache()
        else:
            print_usage(1)

    elif len(sys.argv) > 2:
        print_usage(1)

    try:
        with pid.PidFile(piddir=lock_file_dir, pidname=running_lock_name):
            main()
    except pid.PidFileError:
        log_it("Already running! Quitting ...", 0)
