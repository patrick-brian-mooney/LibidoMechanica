#! /usr/bin/env python3
# -*- coding: utf-8 -*-
"""generate.py creates the content at LibidoMechanica.tumblr.com, which is a
blog consisting of automatically written "love poetry" created by this script.
This program is copyright 2017-18 by Patrick Mooney.


Usage:

    ./generate.py [options]


Options:

    --help, -h      Print this help text, then exit.
    --build, -b     Fully populate the textual similarity cache, then exit.
    --clean, -c     Clean out stale data from the textual similarity cache,
                    then exit.

If no options are given on the command line, the program writes and posts a
poem, then quits.

In a nutshell, this program "writes" these "love poems" by training a Markov
chain-based text generator on a set of existing love poems (a phrase sometimes
rather broadly interpreted) picked from a larger corpus of love and romance
poetry in English. This corpus is itself a work in progress and has known
problems: as of this writing (1 July 2018), for instance, it disproportionately
emphasizes canonical British poets from the 15th to the 18th centuries and
under-represents poems that (for instance) were written in languages other than
English; were written by colonial subjects; were written by working-class
writers; etc. etc. etc. SOME attention (though not enough) has been paid to
such representational matters, and diversifying the corpus of training texts in
many ways is a long-term goal for this project.

The text generator used is a modification of my own Markov chain-based text
generator, forked from Harry R. Schwartz's text generator and heavily modified.
This particular version of the text generator treats individual characters as
the tokens that are mapped, rather than whole words, as my other text-
generation projects do. The generator is trained on a variable series of texts
selected from the corpus, each of which bears SOME computable "similarity" to
another text already selected -- either a seed text from the beginning or
another text previously selected in the same manner. The details and thresholds
for the selection algorithm are currently (25 July 2018) being tweaked quite
regularly.

Once the poem is generated by the text generator, it is modified in various
ways to make it look less like it was generated by an algorithm that was merely
traversing a series of Markov chains randomly. Some of these tasks are:

  * attempting to balance opening and closing punctuation
  * curlifying quotes
  * preventing sentence-ending punctuation from beginning a line
  * stripping spurious line breaks
  * attempting to normalize (according to any of several definitions) stanza
    length.
  * attempting to regularize the number of syllables in a line, according to
    any of several methodologies.

That was not a comprehensive list.

Once the poem is cleaned and otherwise "edited," it is posted to Tumblr, and an
archival copy is posted to the archives/ folder inside the project's folder.
This "archival copy" is a bzipped JSON file recording the poem itself, the
texts used to train the text generator that generated it, and some other info
about the poem's generation. The archives/ folder is periodically cleaned by
hand into a series of folders each containing only 1000 archived poems.

The training-text selection algorithm evaluates the "similarity" between
existing members of the list and candidates for addition to it. This is a
fairly expensive calculation to make, especially when at least one of the texts
being compared is long, and so the results of the calculation are cached
between runs in a global similarity cache. This cache is opened, used and
modified, and then updated on disk when the source text selections have been
made. The class SimilarityCache() is a singleton class: creating one
automatically reads the cache into memory as the _data attribute of the object
being instantiated. Normally, this is probably best done with the convenience
wrapper open_cache(), which is a context manager that ensure the cache is
written back to disk when it is done being used (and has quite likely been
modified). The convenience function clean_cache() cleans stale data out of the
cache; the convenience function build_cache() forces it to be fully populated
with results for all texts in the training corpus (and takes a REALLY LONG TIME
to run if the cache has not already been populated).

This whole script is very much a rough draft and a work in progress. Many of
the sub-tasks that this script accomplishes are accomplished in hacky and
suboptimal ways. There's plenty of room for improvement here.

THIS SOFTWARE IS OFFERED WITHOUT WARRANTY OF ANY KIND AT ALL. It is ALPHA
SOFTWARE; if you don't know what that means, or can't read the source code to
determine whether it meets your needs, then this software is not intended for
you.

Nevertheless, if you want to adapt it, this script is licensed under the GNU
GPL, either version 3, or (at your option) any later version; see the file
LICENSE.md for more details.
"""


import bz2, contextlib, datetime, functools, glob, json, os
import pickle, pprint, random, re, string, sys, time, unicodedata

import pid                                              # https://pypi.python.org/pypi/pid/

from nltk.corpus import cmudict                         # nltk.org

import patrick_logger                                   # https://github.com/patrick-brian-mooney/personal-library
from patrick_logger import log_it

import social_media                                     # https://github.com/patrick-brian-mooney/personal-library
from social_media_auth import libidomechanica_client    # Unshared file that contains authentication constants

import searcher                                         # https://github.com/patrick-brian-mooney/python-personal-library/blob/master/searcher.py

import poetry_generator as pg                           # https://github.com/patrick-brian-mooney/markov-sentence-generator

import text_handling as th                              # https://github.com/patrick-brian-mooney/personal-library


patrick_logger.verbosity_level = 3


home_dir = '/LibidoMechanica'

poetry_corpus = os.path.join(home_dir, 'poetry_corpus')
post_archives = os.path.join(home_dir, 'archives')
similarity_cache_location = os.path.join(home_dir, 'similarity_cache.pkl.bz2')

lock_file_dir = home_dir
running_lock_name = 'running.pid'
updating_lock_name = 'updating.pid'


known_punctuation = string.punctuation + "‘’“”"
syllable_dict = cmudict.dict()


# This next is a global dictionary holding data to be archived at the end of the run. Modified constantly.
post_data = {'tags': ['poetry', 'automatically generated text', 'Patrick Mooney', 'Markov chains'],
             'normalization_strategy': None,
             'syllabic_normalization_strategy': None,
             'stanza length': None,
             }

genny = None            # We'll reassign this soon enough. We want it to be defined in the global namespace, though.


def print_usage(exit_code=0):
    """Print the docstring as a usage message to stdout, then quit with status code
    EXIT_CODE.
    """
    log_it("INFO: print_usage() was called")
    print(__doc__)
    sys.exit(exit_code)


def factors(n):
    """Return a list of the factors of a number. Based on code at
    < https://stackoverflow.com/a/6800214 >.
    """
    assert (int(n) == n and n > 1), "ERROR: factors() called on %s, which is not a positive integer" % n
    return sorted(list(set(functools.reduce(list.__add__, ([i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0)))))


def manually_count_syllables(word):
    """Based on https://datascience.stackexchange.com/a/24865."""
    count = 0
    vowels = 'aeæiouy'
    word = unicodedata.normalize('NFKD', word.lower()).encode('ASCII', 'ignore').decode('ASCII')    # strip diacritics
    if len(word) == 0: return 0             # Naively assume that null words have no syllables.
    if word[0] in vowels:
        count +=1
    for index in range(1,len(word)):
        if word[index] in vowels and word[index-1] not in vowels:
            count +=1
    if word.endswith('e'):
        count -= 1
    if word.endswith('le'):
        count+=1
    if count == 0:
        count +=1
    return count

def syllable_count(word):
    """Do a reasonably good job of determining the number of syllables in WORD, a word
    in English. Uses the CMU corpus if it contains the word, or a best-guess
    approach otherwise. Based on https://stackoverflow.com/a/4103234.
    """
    w = ''.join([c for c in word if c.isalpha()])
    try:
        return sum([len(list(y for y in x if y[-1].isdigit())) for x in syllable_dict[w.lower()]])
    except KeyError:
        return manually_count_syllables(w)

def is_prime(n):
        """Return True if N is prime, false otherwise. "Prime" is here defined specifically
        as "has fewer than three factors," which is not quite the same as mathematical
        ... um, primery. Primeness. Anyway, this is intended to be inclusive about edge
        cases that "is it prime?" should often include (at least for the purposes of
        this particular project) rather than be an exact mathematically correct test.
        """
        assert (int(n) == n and n >= 1), "ERROR: is_prime() called on %s, which is not a positive integer" % n
        return (len(factors(n)) < 3)


def lines_without_stanza_breaks(the_poem):
    """Returns a *list* of lines from THE_POEM, removing any stanza breaks."""
    return [l for l in the_poem.split('\n') if len(l.strip()) > 0]

def total_lines(the_poem):
    """Returns the total number of non-empty lines in the poem."""
    ret = len(lines_without_stanza_breaks(the_poem))
    log_it("    current total lines in poem: %d" % ret, 5)
    return ret


def remove_single_lines(the_poem, combination_probability=0.85):
    """Takes the poem passed as THE_POEM and goes through it, (randomly) eliminating
    single-line stanzas. Returns the modified poem.
    """
    log_it("Removing single lines from the poem with probability %f" % (100 * combination_probability), 3)
    stanzas = [l.split('\n') for l in the_poem.split('\n\n')]   # A list of stanzas, each of which is a list of lines
    i = 0
    while i < len(stanzas):
        if len(stanzas[i]) < 3:
            try:                                # Combine it with the next stanza. Probably.
                if random.random() <= combination_probability:
                    next_stanza = stanzas.pop(i+1)
                    stanzas[i] += next_stanza
                else: i += 1
            except IndexError:                  # If there is no next stanza ...
                if len(stanzas) > 1:            # ... add this stanza to the end of the previous stanza.
                    stanzas[-2] += stanzas.pop()
        else:
            i += 1
    return '\n\n'.join(['\n'.join(s) for s in stanzas])

def regularize_form(the_poem):
    """Tries to find a form that gives THE_POEM a more or less regular syllables-per-
    line pattern. This is another series of rough approximations, of course.

    As it tries various strategies, it attempts to build a FORM list, which is
    simply a list of syllable counts, line by line: so, [10, 10, 10, 10] describes
    a poem with four ten-syllable lines. These numbers are not firm plans, but
    rather a series of rolling goals: what the sample list just given actually
    means, more specifically, is: break off a line with ten or more syllables. Then
    break off a second line that brings the total syllables in the poem to at least
    twenty. Then another line that brings the total syllables to at least thirty.
    Then another line that brings the total syllables to at least forty.

    Once it has made basic choices, it may then go back through and vary the plans,
    using any of several traditional forms as a model for syllabic variation. It
    keeps track of any syllables over the model it has constructed has relative to
    the number of syllables in the source text that is going to be reformatted; at
    the end of the planning process, it randomly removes syllables from anywhere at
    all in the plan in order to make the number fit. As of this writing (25 July
    2018), the total syllabic debt should never exceed one, but this may change in
    the future.

    Once the plans are settled, the poem is made to conform as closely as possible
    to the expected plan without breaking words between lines.

    Once that is done, this function then re-groups the lines according to any of
    several strategies, emphasizing (when possible) regular stanza lengths More
    work is needed to integrate this more tightly with syllabic normalization.

    Once all of this is done, the function returns the modified poem.

    #FIXME: Currently, the algorithm chops off chunks into lines as soon as it
    reaches or exceeds its syllabic goal. However, this has two downsides:
        1. Some zero-syllable-length tokens (e.g., much punctuation) would be
           better placed at the end of the line than at the beginning of the next
           line, but this doesn't happen because we stop as soon as we meet our
           syllabic goal. (We should be looking ahead at the next token.) This is
           why, for instance, some poems generated recently have commas at the
           beginnings of lines.

           * This is definitely not the case with all zero-length punctuation,
             though: the quotation dash should bind right more strongly than it
             binds left, for instance.

        2. We should be considering how far past the current goal the current token
           takes us before deciding whether to incorporate it into the current line,
           rather than just taking whatever's next, no matter how far past the goal
           this puts us. Currently, if we're (say) one syllable short of our current
           syllabic goal, we take a token no matter what the token is; it might
           happen to put us six syllables past our current goal if it's a very long
           word. It would be smarter to scan the token and count syllables before
           making the decision, then base than decision on which choice leaves us
           closer to our current goal at the end of the line.
    """
    global post_data
    log_it("Attempting to regularize poetic form ...", 3)
    form = None
    syllable_debt = 0
    total_syllables = sum([syllable_count(word) for word in genny._token_list(the_poem, character_tokens=False)])
    syllabic_factors = factors(total_syllables)

    # First_attempt: is any of the factors a plausible line length?
    single_line_counts = list(set(syllabic_factors) & set(range(7, 17)))
    if single_line_counts and random.random() < 0.8:
        length = random.choice(single_line_counts)
        form = [length] * (total_syllables // length)
        post_data['syllabic_normalization_strategy'] = 'Regular line length: %d syllables' % length
        log_it("    ... basic strategy: %d syllables per line" % length, 4)
    elif random.random() < 0.4:
        length = total_syllables / the_poem.count('\n')
        form = [length] * the_poem.count('\n')
        post_data['syllabic_normalization_strategy'] = 'Fractional regular line length, based on average: %.4g syllables' % length
        log_it("    ... basic strategy: %f syllables per line" % length, 4)
    elif random.random() < 0.7:
        while not form:
            syllable_debt += 1      # There's a good chance we've already tried, and failed, with zero.
            syllabic_factors = factors(syllable_debt + total_syllables)
            single_line_counts = list(set(syllabic_factors) & set(range(7, 17)))
            if single_line_counts:
                length = random.choice(single_line_counts)
                form = [length] * ((total_syllables + syllable_debt) // length)
                post_data['syllabic_normalization_strategy'] = 'Regular line length: %d syllables, with syllabic debt of %d' % (length, syllable_debt)
        log_it("    ... basic strategy: %d syllables per line, with debt of %d syllables" % (length, syllable_debt), 4)

    # First, vary the form planned, as appropriate. Once again, much more work needed (or at least possible) here.
    log_it("INFO: Deciding whether to apply transformations to basic pattern ...", 3)
    if form:
        count = 0
        if (len(form) % 9 == 0) and (min(form) >= 6) and (random.random() < 0.4):
            log_it("    choosing richatameter as basic modification model", 4)
            post_data['stanza length'] = 9          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (richtameter-like)"
            while count < len(form):
                form[count]   -= (32/9)
                form[count+1] -= (14/9)
                form[count+2] += (4/9)
                form[count+3] += (22/9)
                form[count+4] += (40/9)
                form[count+5] += (22/9)
                form[count+6] += (4/9)
                form[count+7] -= (14/9)
                form[count+8] -= (32/9)
                count += 9
        elif (len(form) % 6 == 0) and (random.random() < 0.5):
            log_it("    choosing Burns stanza as basic modification model", 4)
            if random.random() < 0.8:
                post_data['stanza length'] = 6          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (6-line Burns stanza-like)"
            while count < len(form):
                form[count]   += (4/3)
                form[count+1] += (4/3)
                form[count+2] += (4/3)
                form[count+3] -= (8/3)
                form[count+4] += (4/3)
                form[count+5] -= (8/3)
                count += 6
        elif (len(form) % 5 == 0) and (random.random() < 0.5):
            log_it('    choosing five-line "ballad" as basic modification model', 4)
            if random.random() < 0.7:
                post_data['stanza length'] = 5          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += ' (5-line alternating "ballad" with initial debt of %d)' % syllable_debt
            while count < len(form):
                sign = 1 if syllable_debt <= 0 else -1
                form[count]   += (1 * sign)
                form[count+1] -= (1 * sign)
                form[count+2] += (1 * sign)
                form[count+3] -= (1 * sign)
                form[count+4] += (1 * sign)
                count += 5
                syllable_debt += sign
        elif (len(form) % 5 == 0) and (random.random() < 0.25):
            log_it("    choosing cinquain as basic modification model", 4)
            if random.random() < 0.7:
                post_data['stanza length'] = 5          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (cinquain-like)"
            while count < len(form):
                form[count]   -= 2.4
                form[count+1] -= 0.4
                form[count+2] += 1.6
                form[count+3] += 3.6
                form[count+4] -= 2.4
                count += 5
        elif (len(form) % 4 == 0) and (random.random() < 0.4):
            log_it('    choosing "ballad" with short last line with as basic modification model', 4)
            if random.random() < 0.6:
                post_data['stanza length'] = 4          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (4-line ballad-like with short last line)"
            while count < len(form):
                form[count] += 1
                form[count + 2] += 1
                form[count + 3] -= 2
                count += 4
        elif (len(form) % 3 == 0 and (random.random() < 0.333333)):
            log_it("    choosing haiku as basic modification model", 4)
            if random.random() < 0.4:
                post_data['stanza length'] = 3          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (3-line haiku-like)"
            while count < len(form):
                form[count] -= 1
                form[count + 1] += 2
                form[count + 2] -= 1
                count += 3
        elif (len(form) % 3 == 0 and (random.random() < 0.333333)):
            log_it("    choosing short-middle-line terza rima as basic modification model", 4)
            post_data['stanza length'] = 3          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (3-line terza rima-like with short middle line)"
            while count < len(form):
                form[count] += 1
                form[count + 1] -= 2
                form[count + 2] += 1
                count += 3
        elif (len(form) % 2 == 0) and (random.random() < 0.6):
            log_it("    choosing two-line ballad-like stanza as basic modification model", 4)
            if random.random() < 0.3:
                post_data['stanza length'] = 2          # Force regular-number-of-lines stanzas as form.
            post_data['syllabic_normalization_strategy'] += " (2-line ballad-like)"
            while count < len(form):
                form[count] += 1
                form[count+1] -= 1
                count += 2

        # Next, pay off any syllabic debt.
        if syllable_debt:
            log_it("... paying off syllabic debt", 3)
            for i in random.sample(range(len(form)), syllable_debt):    # Choose random lines from the poem
                log_it("    ... removing one syllable from line %d" % i, 5)
                form[i] -= 1                                            # ... they can pay off the debt.
        post_data['form_plan'] = '[' + ', '.join('%.6g' % i for i in form) + ']'

        # OK, now make the poem conform (approximately) to the form we planned.
        log_it("INFO: re-breaking poetic lines ...", 4)
        tokenized_poem = genny._token_list(the_poem, character_tokens=False)
        working_copy = ''.join(list(the_poem))
        lines, total_syllables = [][:], 0
        current_line = ''
        current_goal = form.pop(0)
        while tokenized_poem:
            current_token = tokenized_poem.pop(0)
            current_token_with_context = working_copy[:working_copy.find(current_token) + len(current_token)]
            if current_token_with_context.count('\n'):
                current_token_with_context = th.multi_replace(current_token_with_context, [['\n', ' ']])
            working_copy = working_copy[len(current_token_with_context):]
            current_line += current_token_with_context
            total_syllables += syllable_count(current_token)
            if total_syllables >= current_goal:         # We've (probably) hit a line break. Reset the things that need to be reset.
                if (not tokenized_poem) or (syllable_count(tokenized_poem[0]) != 0):      # If there are zero-syllable tokens coming up, don't break yet.
                    lines += [current_line + '\n' if not current_line.endswith('\n') else ""]
                    current_line = ''
                    try:
                        current_goal += form.pop(0)
                    except IndexError:                  # No more lines left? Just run to end of poem.
                        current_goal += sum([syllable_count(word) for word in tokenized_poem])
        the_poem = ''.join(lines)

    # OK, now that we've rearranged words from one line to another, we modify the overall stanza form of the poem.
    textual_lines = lines_without_stanza_breaks(the_poem)
    post_data['normalization_strategy'] = None
    if post_data['stanza length'] or ((not is_prime(len(textual_lines))) and (random.random() < 0.8)):
        if not post_data['stanza length']:
            post_data['normalization_strategy'] = 'regular stanza length'
            possible_stanza_lengths = factors(len(textual_lines))
            if len([x for x in possible_stanza_lengths if x >= 3]):     # If possible, prefer stanzas at least as long as Dante's in the Divine Comedy.
                possible_stanza_lengths = [x for x in possible_stanza_lengths if x >= 3]
            if len([x for x in possible_stanza_lengths if x <= 16]):    # If possible, choose a stanza length no longer than Meredith's extended sonnets in /Modern Love/.
                possible_stanza_lengths = [x for x in possible_stanza_lengths if x <= 16]
            post_data['stanza length'] = random.choice(possible_stanza_lengths)
            if post_data['stanza length'] == len(textual_lines):
                pass
            if post_data['stanza length'] == 1:
                post_data['stanza length'] = len(textual_lines)      # 1 long stanza, not many one-line stanzas
        the_poem = ""
        for stanza in range(0, len(textual_lines) // post_data['stanza length']):  # Iterate over the appropriate # of stanzas
            for line in range(0, post_data['stanza length']):
                the_poem += "%s\n" % textual_lines.pop(0)
            the_poem += '\n'  # Add stanza break
    elif (not the_poem.count('\n\n')) and (random.random()) < 0.85:
        post_data['normalization_strategy'] = 're-introduce random stanza breaks'
        poem_lines = the_poem.split('\n')
        for i in range(0, 1 + random.randint(1, len(textual_lines) // 3)):
            poem_lines[random.randint(0, len(poem_lines)-1)] += '\n'
        the_poem = '\n'.join(poem_lines)
    elif random.random() < 0.8:
        post_data['normalization_strategy'] = 'remove single lines (strict)'
        the_poem = remove_single_lines(the_poem, combination_probability=1)
    if (post_data['normalization_strategy'] == 're-introduce random stanza breaks') or (post_data['normalization_strategy'] == None and (random.random() < 0.8)):
        if post_data['normalization_strategy'] == 're-introduce random stanza breaks':
            post_data['normalization_strategy'] += ' + remove single lines (lax)'
        else:
            post_data['normalization_strategy'] = 'remove single lines (lax)'
        the_poem = remove_single_lines(the_poem, combination_probability=0.9)
    return the_poem


def count_previous_untitled_poems():
    ret = len([x for x in searcher.get_files_list(post_archives,None) if 'untitled' in x.lower()])
    log_it("Counted previous untitled poems: %d" % ret, 4)
    return ret

def fix_punctuation(the_poem):
    """Cleans up the punctuation in the poems so that it appears to be more
    'correct.' Since characters are generated randomly based on a frequency
    analysis of which characters are likely to follow the last three to ten
    characters, there's no guarantee that (for instance) parentheses or quotes
    are balanced, because the generator doesn't pay attention to or understand
    larger-scale structures.

    THE_POEM is a string, which is the text of the entire poem; the function
    returns a new, punctuation-fixed version of the poem passed in.

    NOT YET FULLY IMPLEMENTED.
    #FIXME: balancing punctuation needs to stop using heuristics and accurately
            check structure.
    #FIXME: we need to deal with the single/double quotes nesting problem.
    """
    log_it("INFO: about to alter punctuation", 2)
    the_poem = strip_invalid_chars(the_poem)
    the_poem = curlify_quotes(the_poem, "'", "‘", "’")
    the_poem = curlify_quotes(the_poem, '"', '“', '”')
    the_poem = balance_punctuation(the_poem, "‘", "’")
    the_poem = balance_punctuation(the_poem,  '“', '”')
    the_poem = balance_punctuation(the_poem,  '(', ')')
    the_poem = balance_punctuation(the_poem,  '[', ']')
    return balance_punctuation(the_poem,  '{', '}')

def get_title(the_poem):
    """Get a title for the poem. There are several title-generating algorithms; this
    function picks one at random.
    """
    log_it("INFO: getting a title for the poem", 2)
    if random.random() < (1/15):
        title = "Untitled Poem # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (1/14):
        title = "Untitled Composition # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (1/13):
        title = "Untitled # %d" % (1 + count_previous_untitled_poems())
    elif random.random() < (2/12):
        title = "Untitled (‘%s’)" % th.strip_leading_and_trailing_punctuation(the_poem.split('\n')[0]).strip()
    elif random.random() < (4/10):          # First line, in quotes
        title = th.strip_leading_and_trailing_punctuation(the_poem.strip().split('\n')[0]).strip()
    elif random.random() < (3/6):           # Pick one of the first three lines
        title = "‘%s’" % th.strip_leading_and_trailing_punctuation(the_poem.split('\n')[random.randint(1,4)-1]).strip()
    else:                                   # New 'sentence'
        title = th.strip_leading_and_trailing_punctuation(genny.gen_text(sentences_desired=1).split('\n')[0].strip())
    if len(title) < 5:                              # Try again, recursively.
        title = get_title(the_poem)
    while len(title) > 70:
        words = title.split()
        title = ' '.join(words[:random.randint(3, min(12, len(words)))])
    title = title.strip()
    if title.startswith('‘') and not title.endswith('’'):       # The shortening procedure above might have stripped the closing quote
        title = title + '’'
    if '(‘' in title and not '’)' in title:
        title = title + '’)'
    title = fix_punctuation(title)
    log_it("Title is: %s" % title)
    return title


def strip_invalid_chars(the_poem):
    """Some characters appear in the training texts but are characters that, I am
    declaring by fiat, should not make it into the final generated poems at all.
    The underscore is a good example of characters in this class. This function
    takes an entire poem as input (THE_POEM) and returns a poem entirely
    stripped of all such characters.
    """
    log_it("INFO: stripping invalid characters from the poem", 2)
    invalids = ['_', '*']
    return ''.join([s for s in the_poem if not s in invalids])

def curlify_quotes(the_poem, straight_quote, opening_quote, closing_quote):
    """Goes through THE_POEM (a string) and looks for instances of STRAIGHT_QUOTE
    (a single-character string). When it finds these instances, it substitutes
    OPENING_QUOTE or CLOSING_QUOTE for them, trying to make good decisions about
    which of those substitutions is appropriate.

    IMPORTANT CAVEAT: this routine iterates over THE_POEM, making in-place
    changes at locations determined via an initial scan. This means that
    OPENING_QUOTE and CLOSING_QUOTE **absolutely must** have the same len() as
    STRAIGHT_QUOTE, or else weird things will happen. This should not be a
    problem with standard English quotes under Python 3.X; but it may fail under
    non-Roman scripts, in odd edge cases, or if the function is used to try to
    do something other than curlify quotes.

    NOT FULLY TESTED, but I'm going to bed.
    """
    log_it("INFO: curlify_quotes() called to differentiate %s (%d) into %s and %s" % (straight_quote, the_poem.count(straight_quote), opening_quote, closing_quote), 2)
    assert len(straight_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    assert len(opening_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    assert len(closing_quote) == 1, "Quote characters passed to curlify_quotes() must be one-character strings"
    index = 0
    while index < len(the_poem):
        index = the_poem.find(straight_quote, index)
        if index == -1:
            break       # We're done.
        if index == 0:                                                      # Is it the first character of the poem?
            the_poem = opening_quote + the_poem[1:]
        elif index == len(the_poem):                                        # Is it the last character of the poem?
            the_poem = the_poem[:-1] + closing_quote
        elif the_poem[index-1].isspace() and the_poem[index+1].isspace():   # Whitespace on both sides? Replace quote with space.
            the_poem = the_poem[:index] + ' ' + the_poem[1+index:]
        elif not the_poem[index-1].isspace():                               # Non-whitespace immediately before quote? It's a closing quote.
            the_poem = the_poem[:index] + closing_quote + the_poem[index+1:]
        elif not the_poem[index+1].isspace():                               # Non-whitespace just after quote? It's an opening quote.
            the_poem = the_poem[:index] + opening_quote + the_poem[index+1:]
        else:                                                               # Quote appears in middle of non-whitespace text ...
            if straight_quote == '"':
                the_poem = the_poem[:index-1] + the_poem[index+1:]                  # Just strip it out.
            elif straight_quote == "'":
                the_poem = the_poem[:index-1] + closing_quote + the_poem[index+1:]  # Make it an apostrophe.
            else:
                raise NotImplementedError("We don't know how to deal with this quote: %s " % straight_quote)
    return the_poem

def balance_punctuation(the_poem, opening_char, closing_char):
    """Makes sure that paired punctuation (smart quotes, parentheses, brackets) in the
    poem are 'balanced.' If not, it attempts to correct it.
    """
    opening, closing = the_poem.count(opening_char), the_poem.count(closing_char)
    if closing_char == '’':     # Sigh. We have to worry about apostrophes that look like closing single quotes.
        closing -= len(re.findall('[:alnum:]*’[:alnum:]', the_poem))    # Inside a word? It's an apostrophe. Don't count it

    log_it("INFO: Balancing %s and %s (%d/%d)" % (opening_char, closing_char, opening, closing), 2)

    if opening or closing:      # Do nothing if there's no instances of either character
        if opening != closing:  # Do nothing if we already have equal numbers (even if not properly "balanced")
            nesting_level = 0   # How many levels deep are we right now in the punctuation we're tracking?
            indexed_poem = list(the_poem)
            index = 0
            while index <= (len(indexed_poem)-1):
                char = indexed_poem[index]
                next_char = '' if index == len(indexed_poem) -1 else indexed_poem[index + 1]
                last_char = '' if index == 0 else indexed_poem[index - 1]
                if index == (len(indexed_poem)-1) :  # End of the poem?
                    if nesting_level > 0:               # Close any open characters.
                        indexed_poem += [closing_char]
                        nesting_level -= 1
                    index += 1
                elif char == opening_char:          # Opening character?
                    if index == len(indexed_poem):      # Last character is an opening character?
                        indexed_poem.pop(-1)            # Just drop it.
                    else:
                        nesting_level += 1              # We're one level deeper
                        index += 1                          # Move on to next character
                elif char == closing_char:          # Closing character?
                    if (closing_char == '’') and (th.is_alphanumeric(next_char) and th.is_alphanumeric(last_char)):
                        index += 1      # Skip apostrophes in the middle of words
                    else:
                        if nesting_level < 1:               # Are we trying to close something that's not open?
                            indexed_poem.pop(index)         # Just drop the spurious close quote
                        else:
                            if next_char.isspace():             # Avoid non-quote apostrophes in middle of words.
                                nesting_level -= 1          # We're one level less deep
                            index += 1                      # Move on to next character
                elif nesting_level > 0:             # Are we currently in the middle of a bracketed block?
                    if next_char.isspace():             # Is the next character whitespace?
                        if random.random() < (0.001 * nesting_level):   # Low chance of closing the open bracket
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    elif char in ['.', '?', '!'] and next_char.isspace():
                        if random.random() < (0.05 * nesting_level):    # Higher chance of closing the open bracketer
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                            if random.random() < 0.2:                       # Force new paragraph break?
                                indexed_poem.insert(index + 1, '\n')
                    elif char in known_punctuation and last_char in ['.', '!', '?']:
                        if random.random() < (0.05 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                            if random.random() < 0.2:                       # Force new paragraph break?
                                indexed_poem.insert(index + 1, '\n')
                    elif char == '\n' and next_char == '\n':            # Very high chance of closing on paragraph boundaries
                        if random.random() < (0.4 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    elif char == '\n':
                        if random.random() < (0.1 * nesting_level):
                            indexed_poem.insert(index, closing_char)
                            nesting_level -= 1
                    index += 1
                else:
                    index += 1
            the_poem = ''.join(indexed_poem)
    log_it("   ... after balancing, there are %d/%d punctuation marks." % (the_poem.count(opening_char), the_poem.count(closing_char)), 3)
    return the_poem

def do_basic_cleaning(the_poem):
    """Does first-pass elementary cleanup tasks on THE_POEM. Returns the cleaned
    version of THE_POEM.
    """
    log_it("INFO: about to do basic pre-cleaning of poem", 2)
    the_poem = th.multi_replace(the_poem, [[' \n', '\n'], ['\n\?', '?'], ['\n!', '!'],
                                           ['\n"', '\n'], ['\n”', '\n'], ['\n\n\n', '\n\n'],
                                           ['\n" ', '\n"'], ['^" ', '"']]).strip()
    return the_poem


def do_final_cleaning(the_poem):
    log_it("INFO: about to do final cleaning of poem", 2)
    the_poem = th.multi_replace(the_poem, [[' \n', '\n'],               # Eliminate any spurious end-of-line spaces
                                           ['\n\n\n', '\n\n'],          # ... and any extra line breaks.
                                           [r'\n\)', r')'],             # And line breaks right before ending punctuation
                                           ['\n\?', '?'], ['\n!', '!'],
                                           ['\n"', '\n'], ['\n”', '\n'], ['\n’', '’'],
                                           ['“\n', '“'], ['"\n', '"'],  # And line breaks right after beginning punctuation
                                           ['\n—\n', '—\n'],            # Don't allow an em dash to be the only character on a line.
                                          ])
    poem_lines = the_poem.split('\n')
    index = 0
    while index < (len(poem_lines) - 1):                                # Go through, line by line, making any final changes.
        line = poem_lines[index]
        if '  ' in line.strip():                                        # Multiple whitespace in line? Break into multiple lines
            individual_lines = ['  ' + i + '\n' for i in line.split('  ')]
            if len(individual_lines) > 1:
                poem_lines.pop(index)
                individual_lines.reverse()          # Go through the sub-lines backwards,
                for l in individual_lines:
                    poem_lines.insert(index, l)     # ... inserting lines and pushing the line stack up.
            index += len(individual_lines)
        else:
            index += 1
    the_poem = '\n'.join(poem_lines)
    return regularize_form(the_poem)


@functools.lru_cache(maxsize=8)
def get_mappings(f, markov_length):
    """Trains a generator, then returns the calculated mappings."""
    log_it("get_mappings() called for file %s" % f, 5)
    return pg.PoemGenerator(training_texts=[f], markov_length=markov_length).chains.the_mapping


class SimilarityCache(object):
    """This class is an object that manages the global cache of text similarities.

    The object's internal data cache is a dictionary:
        { (text_name_one, text_name_two):         (a tuple)
              { 'when':,                          (a datetime: when the calculation was made)
                'similarity':                     (a value between 0 and 1, rather heavily weighted toward zero)
                  }
             }
    """
    def __init__(self, cache_file=similarity_cache_location):
        try:
            with bz2.open(similarity_cache_location, "rb") as pickled_file:
                log_it("Loading cached similarity data ...", 3)
                self._data = pickle.load(pickled_file)
        except (OSError, EOFError, AttributeError, pickle.PicklingError) as err:
            log_it("WARNING! Unable to load cached similarity data because %s. Preparing empty cache ..." % err)
            self._data = dict()
        self._dirty = False
        self._cache_file = cache_file

    def __str__(self):
        try:
            return "< Textual Similarity Cache, with %d results cached >" % len(self._data)
        except AttributeError:
            return "< Textual Similarity Cache (not fully initialized: no data attached) >"
        except BaseException as err:
            return "< Textual Similarity Cache (unknown state because %s) >" % err

    def flush_cache(self):
        """Writes the textual similarity cache to disk, if self._dirty is True. If
        self._dirty is False, it silently returns without doing anything.

        Or, rather, that's the basic idea. In fact, what it does is reload the version
        of the cache that's currently on disk and updates it with new info instead of
        replacing the one on disk. The reason for this, of course, is that this
        script has become complex enough that it may take more than an hour to run on
        the slow old laptop that hosts it ... and so there may be multiple copies
        running, each of which thinks it has the "master copy" in memory. To help
        ameliorate the potential for race conditions, we update instead of overwriting.

        #FIXME: This function does not do any file locking; there's nothing preventing
        multiple attempts to update the cache at the same time. The convention for
        reducing this problem is that any code, before calling flush_cache(), must
        acquire a PidFile lock distinct from the one the main script acquires before
        beginning its run. Code that needs to write to the cache needs to repeatedly
        attempt to acquire this lock, waiting in between failed attempts, until it is
        able to do so. See .build_cache() for an example.

        In fact, we should be using some sort of real database-like thing, because the
        overhead of keeping all this data in memory could in theory grow quite large.
        """
        if not self._dirty:
            log_it("Skipping cache update: no changes made!", 4)
            return
        log_it("Updating similarity data cache on disk ...", 3)
        try:
            with bz2.open(self._cache_file, "rb") as pickled_file:
                old = pickle.load(pickled_file)
            old.update(self._data)
            self._data = old
        except (OSError, EOFError, pickle.PicklingError) as err:
            log_it("Not able to update previous data: %s" % err)
        except BaseException as err:
            log_it("Unhandled exception occurred!   %s" % err)
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._data, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
        log_it(" ... updated!", 3)
        self._dirty = False

    @staticmethod
    def calculate_overlap(one, two):
        """return the percentage of chains in dictionary ONE that are also in
        dictionary TWO.
        """
        overlap_count = 0
        for which_chain in one.keys():
            if which_chain in two: overlap_count += 1
        return overlap_count / len(one)

    def calculate_similarity(self, one, two, markov_length=5):
        """Come up with a score evaluating how similar the two texts are to each other.
        This actually means, more specifically, "the product of (a) the percentage of
        chains in the set of chains of length MARKOV_LENGTH constructed from text ONE
        that are also in text TWO; multiplied by (b) the percentage of chains of
        length MARKOV_LENGTH constructed from text TWO that are also in chains
        constructed from text ONE.

        This routine also caches the calculated result in the global similarity cache.
        It's a comparatively expensive calculation to make, so we store the results.
        """
        log_it("calculate_similarity() called for: %s" % [one, two], 5)
        if one == two:
            return 1                        # Well, that's easy.
        chains_one = get_mappings(one, markov_length)
        chains_two = get_mappings(two, markov_length)
        ret = self.calculate_overlap(chains_one, chains_two) * self.calculate_overlap(chains_two, chains_one)
        self._data[tuple(sorted([os.path.basename(one), os.path.basename(two)]))] = {'when': time.time(), 'similarity': ret,}
        self._dirty = True
        return ret

    def get_similarity(self, one, two):
        """Checks to see if the similarity between ONE and TWO is already known. If it is,
        returns that similarity. Otherwise, calculates the similarity and stores it in
        the global similarity cache, which is written at the end of the script's run.

        In short, this function takes advantage of the memoization of
        calculate_similarity, also taking taking advantage of the fact that
        calculate_similarity(A, B) = calculate_similarity(B, A). It also watches to make
        sure that neither of the texts involved has been changed since the calculation
        was initially made. If either has, it re-performs the calculation and stores
        the updated result in the cache.

        Note that calculate_similarity() itself stores the results of the function. This
        function only takes advantage of the stored values.
        """
        # Index in lexicographical order, by actual filename, after dropping path
        index = tuple(sorted([os.path.basename(one), os.path.basename(two)]))
        log_it("get_similarity() called for files: %s" % list(index), 5)

        if index in self._data:                       # If it's in the cache, and the data isn't stale ...
            if self._data[index]['when'] < os.path.getmtime(one):
                log_it("  ... but cached data is stale relative to %s !" % one, 6)
                return self.calculate_similarity(one, two)
            if self._data[index]['when'] < os.path.getmtime(two):
                log_it("  ... but cached data is stale relative to %s !" % two, 6)
                return self.calculate_similarity(one, two)
            log_it(" ... returning cached value!", 6)
            return self._data[index]['similarity']

        log_it(" ... not found in cache! Calculating and cacheing ...", 6)
        return self.calculate_similarity(one, two)

    def build_cache(self):
        """Sequentially go through the corpus, text by text, forcing comparisons to all
        other texts and cacheing the results, to make sure the cache is fully
        populated. Periodically, it dumps the results to disk by updating the on-disk
        cache, so that not all of the calculation results are lost if the run is
        interrupted. Note that "saves" are actually updates, because it's possible that
        there are multiple instances of this script running -- say, one that is
        intentionally running this function in an IDE, and another that's run
        automatically as a CRON job to generate a poem, but which also winds up
        calculating new similarity data that it also saves. Concurrent runs do not
        share the in-memory copy of the cache, and the second ("generating") run
        mentioned above may wind up duplicating the work of the first. Hence "update,"
        above. The worst-case scenario here is a corrupted cache, which SHOULD be newly
        created by script runs. Hopefully. In any case, if not, hey, it's just a cache.

        This method takes a VERY long time to run if starting from an empty cache with
        many source texts in the corpus. The cache CAN OF COURSE be allowed to
        populate itself across multiple runs.
        """
        log_it("Building cache ...")
        for i, first_text in enumerate(sorted(glob.glob(os.path.join(poetry_corpus, '*')))):
            if i % 10 == 0:
                log_it("  We've performed full calculations for %d texts!" % i)
                if i % 400 == 0:
                    while self._dirty:
                        try:
                            with pid.PidFile(piddir=home_dir):
                                self.flush_cache()                          # Note that success clears self._dirty.
                        except pid.PidFileError:
                            time.sleep(5)                   # In use? Wait and try again.
            for j, second_text in enumerate(sorted(glob.glob(os.path.join(poetry_corpus, '*')))):
                log_it("About to compare %s to %s ..." % (os.path.basename(first_text), os.path.basename(second_text)), 6)
                _ = self.get_similarity(first_text, second_text)

    def clean_cache(self):
        """Run through the cache, checking for problems and fixing them. Work on a copy
        of the data, then rebind the copy over the original data after cleaning is
        done.
        """
        pruned = self._data.copy()
        for count, (one, two) in enumerate(self._data):
            try:
                if count % 1000 == 0:
                    log_it("We're on entry # %d: that's %d %% done!" % (count, (100 * count/len(self._data))))
                assert os.path.isfile(os.path.join(poetry_corpus, one)), "'%s' does not exist!" % one
                assert os.path.isfile(os.path.join(poetry_corpus, two)), "'%s' does not exist!" % two
                assert one <= two, "%s and %s are mis-ordered!" % (one, two)
                assert self._data[(one, two)]['when'] >= os.path.getmtime(os.path.join(poetry_corpus, one)), "data for '%s' is stale!" % one
                assert self._data[(one, two)]['when'] >= os.path.getmtime(os.path.join(poetry_corpus, two)), "data for '%s' is stale!" % two
                _ = int(self._data[(one, two)]['when'])
            except (AssertionError, ValueError, KeyError) as err:
                log_it("Removing entry: (%s, %s)    -- because: %s" % (one, two, err))
                del pruned[(one, two)]
                self._dirty = True
            except BaseException as err:
                log_it("Unhandled error: %s! Leaving data in place" % err)
        removed = len(self._data) - len(pruned)
        log_it("Removed %d entries; that's %d %%!" % (removed, 100 * removed/len(self._data)))
        self._data = pruned
        # We're now going to flush the newly cleaned cache directly to disk. Note that
        # we're not UPDATING THE CACHE using flush_cache(), because that would just
        # allow stale old data that we just cleaned out to propagate back in.
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._data, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
            self._dirty = False
            log_it("Cache updated!")

def old_selection_method(available):
    """This is the original selection method, which merely picks a set of texts at
    random from the corpus. It is fast, but makes no attempt to select texts that
    are similar to each other. This method of picking training texts often produces
    poems that "feel disjointed" and that contain comparatively longer sections of
    continuous letters from a single source text.
    """
    global post_data
    log_it(" ... according to the old (pure random choice) method")
    post_data['tags'] += ['old textual selection method']
    return random.sample(available, random.randint(100, 300))

def new_selection_method(available, similarity_cache):
    """The "new method" for choosing source texts involves picking a small number of
    seed texts completely at random, then going through and adding to this small
    corpus by looking for "sufficiently similar" texts to texts already in the
    corpus. "Similarity" is here defined as "having a comparatively high number
    of overlapping chains" as the text it's being compared to. A text as similarity
    1.0 when compared to itself, and similarity 0.0 when it is compared to a text
    that generates no chains in common with it (something in a different script,
    say). Typically, two poems in English chosen more or less at random will have a
    similarity score in the range of .015 to .07 or so.

    Given the initial seed set, then, each poem not already in the set is considered
    sequentially. "Considered" here means that each poem in the already-selected
    corpus is given a chance to "grab" the poem under consideration; the more
    similar the two poems are, the more likely the already-in-the-corpus poem is to
    "grab" the new poem. This process repeats until "there are enough" poems in the
    training corpus.

    This is a slow process: it can take several minutes even on my faster computer.
    Because the similarity calculations are comparatively slow, but many of them
    must be performed to choose a set of training poems, the results of the
    similarity calculations are stored in a persistent cache of similarity-
    calculation results between runs.
    """
    global post_data
    ret = random.sample(available, random.randint(3, 7))  # Seed the pot with several random source texts.
    for i in ret: available.remove(i)  # Make sure already-chosen texts are not chosen again.
    done, candidates = False, 0
    announced, last_count = set(), 0
    while not done:
        candidates += 1
        if not available:
            available = [f for f in glob.glob(os.path.join(poetry_corpus, '*')) if not os.path.isdir(f) and f not in ret]   # Refill the list of options if we've rejected them all.
        current_choice = random.choice(available)
        available.remove(current_choice)
        changed = False
        for i in ret:  # Give each already-chosen text a chance to "claim" the new one
            if random.random() < (similarity_cache.get_similarity(i, current_choice) / len(ret)):
                ret += [current_choice]
                changed = True
                break
        if candidates > 10000 and len(ret) >= 75:
            done = True
        if candidates % 5 == 0:
            log_it("    ... %d selection candidates" % candidates, 4)
            if changed:
                if (1 - random.random() ** 4.5) < ((len(ret) - 160) / 250):
                    done = True
        if candidates % 25 == 0:
            if len(ret) > last_count:
                log_it("  ... %d selected texts in %d candidates. New: %s" % (len(ret), candidates, {os.path.basename(f) for f in set(ret) ^ announced}), 3)
                announced, last_count = set(ret), len(ret)
            else:
                log_it("  ... %d selected texts in %d candidates" % (len(ret), candidates), 3)
    post_data["rejected training texts"] =  candidates - len(ret)
    if similarity_cache._dirty: similarity_cache.flush_cache()
    return ret


oldmethod = True               # Set to True when tweaking the newer method to use the old method as a fallback.
def get_source_texts(similarity_cache):
    """Return a list of partially randomly selected texts to serve as the source texts
    for the poem we're writing. There are currently two textual selection methods,
    called "the old method" and "the new method." Each is documented in its own
    function docstring.
    """
    log_it("Choosing source texts")
    available = [f for f in glob.glob(os.path.join(poetry_corpus, '*')) if not os.path.isdir(f)]
    if oldmethod:
        return old_selection_method(available)
    else:
        return new_selection_method(available, similarity_cache)


@contextlib.contextmanager
def open_cache():
    """A context manager that returns the persistent similarity cache and closes it,
    updating it if necessary, when it's done being used. This function repeatedly
    attempts to acquire exclusive access to the cache until it is successful at
    doing so, checking the updating_lock_name lock until it can acquire it.
    """
    opened = False
    while not opened:
        try:
            with pid.PidFile(piddir=lock_file_dir, pidname=updating_lock_name):
                similarity_cache = SimilarityCache()
                yield similarity_cache
                opened = True
                similarity_cache.flush_cache()
        except pid.PidFileError:                        # Already in use? Wait and try again.
            time.sleep(10)


def main():
    global genny, post_data

    if not oldmethod:
        with open_cache() as similarity_cache:
            sample_texts = get_source_texts(similarity_cache)
    else:
        sample_texts = get_source_texts(None)
    log_it(" ... selected %d texts" % len(sample_texts), 2)

    # Next, set up the basic parameters for the run
    chain_length = round(min(max(random.normalvariate(6, 0.8), 3), 10))

    # And track sources of this particular poem
    source_texts = [ th.remove_prefix(os.path.basename(t), "Link to ").strip() for t in sample_texts ]
    post_data['tags'] += ['Markov chain length: %d' % chain_length, '%d texts' % len(sample_texts) ]

    poem_length = round(min(max(random.normalvariate(10, 5), 4), 200))          # in SENTENCES. Not lines.

    log_it("INFO: about to set up and train text generator ...")
    genny = pg.PoemGenerator(name='Libido Mechanica poetry generator', training_texts=sample_texts, markov_length=chain_length)
    log_it(" ...trained!")

    log_it("INFO: about to generate poem ...")
    raw_poem = genny.gen_text(sentences_desired=poem_length, paragraph_break_probability=0.2)

    post_data['title'] = get_title(raw_poem)

    log_it("poem generated; title is: %s" % post_data['title'])
    log_it("lines are: \n\n" + raw_poem)
    log_it("tags are: %s" % post_data['tags'])

    log_it("INFO: cleaning poem up ...")
    the_poem = do_basic_cleaning(raw_poem)
    the_poem = fix_punctuation(the_poem)
    the_poem = do_final_cleaning(the_poem)

    log_it("INFO: HTML-izing poem ...")
    # Force all spaces to be non-breaking spaces
    formatted_poem = th.multi_replace(the_poem, [[' ', '&nbsp;']])
    # Add HTML <br /> to end of every line
    formatted_poem = '\n'.join([line.rstrip() + '<br />' for line in the_poem.split('\n')])
    # Wrap stanzas in <p> ... </p>
    formatted_poem = '\n'.join(['<p>%s</p>' % line for line in formatted_poem.split('<br />\n<br />')])
    # Eliminate extra line breaks at the very beginning of paragraphs
    formatted_poem = th.multi_replace(formatted_poem, [['<p><br />\n', '<p>'], ['<p>\n', '<p>'], ['<p>\n', '<p>']])
    # Pretty-print (for debugging only; doesn't matter for Tumblr upload, but neither does it cause problems)
    formatted_poem = th.multi_replace(formatted_poem, [['<p>\n', '\n<p>']])
    # Prevent all spaces from collapsing; get rid of spurious paragraphs
    formatted_poem = th.multi_replace(formatted_poem, [['<p></p>', ''], ['<p>\n</p>', '']])
    log_it('INFO: Attempting to post the content...')
    post_data['status_code'], post_data['tumblr_data'] = social_media.tumblr_text_post(libidomechanica_client, ', '.join(post_data['tags']), post_data['title'], formatted_poem)
    log_it('INFO: the_status is: ' + pprint.pformat(post_data['status_code']), 2)
    log_it('INFO: the_tumblr_data is: ' + pprint.pformat(post_data['tumblr_data']), 3)

    log_it("INFO: archiving poem and metadata ...")
    post_data['text'], post_data['time'] = the_poem, datetime.datetime.now().isoformat()
    post_data['raw_poem'], post_data['formatted_text'] = raw_poem, formatted_poem
    post_data['sources'] = sorted(source_texts)
    archive_name = "%s — %s.json.bz2" % (post_data['time'], post_data['title'])
    with bz2.BZ2File(os.path.join(post_archives, archive_name), mode='wb') as archive_file:
        archive_file.write(json.dumps(post_data, sort_keys=True, indent=3, ensure_ascii=False).encode())
    log_it("INFO: We're done")


def clean_cache():
    """Clean out the existing file similarity cache, then quit."""
    with open_cache() as similarity_cache:
        similarity_cache.clean_cache()
    sys.exit(0)


def build_cache():
    """Clean out the existing file similarity cache, then make sure it's fully populated."""
    with open_cache() as similarity_cache:
        similarity_cache.clean_cache()
        similarity_cache.build_cache()
    sys.exit(0)


force_cache_update = False                  # Set this to True when needing to step through these in an IDE.
if force_cache_update:
    build_cache()


if __name__ == "__main__":
    if len(sys.argv) == 2:
        if sys.argv[1] in ['--help', '-h']:
            print_usage()
        elif sys.argv[1] in ['--clean', '-c']:
            clean_cache()
        elif sys.argv[1] in ['--build', '-b']:
            build_cache()
        else:
            print_usage(1)
    elif len(sys.argv) > 2:
        print_usage(1)

    try:
        with pid.PidFile(piddir=lock_file_dir, pidname=running_lock_name):
            main()
    except pid.PidFileError:
        log_it("Already running! Quitting ...", 0)
