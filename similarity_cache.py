#! /usr/bin/env python3
# -*- coding: utf-8 -*-
"""similarity_cache.py is a utility class for Patrick Mooney's LibidoMechanica
project. It provides a class that tracks calculated "similarity" between texts
in the poetry corpus. This is an expensive calculation whose value needs to be
known repeatedly over multiple runs of the script, so it's saved once made.

This file is part of the LibidoMechanica scripts, a project that is copyright
2016-19 by Patrick Mooney. It is alpha software and the author releases it
ABSOLUTELY WITHOUT WARRANTY OF ANY KIND. You are welcome to use it under the
terms of the GNU General Public License, either version 3 or (at your option)
any later version. See the file LICENSE.md for details.
"""


import bz2, functools, glob, os, pickle, time

import pid                                              # https://pypi.python.org/pypi/pid/
import pandas as pd                                     # https://pandas.pydata.org/


from utils import *
import poetry_generator as pg

from patrick_logger import log_it       # https://github.com/patrick-brian-mooney/personal-library


@functools.lru_cache(maxsize=8)
def get_mappings(f, markov_length):
    """Trains a generator, then returns the calculated mappings."""
    log_it("get_mappings() called for file %s" % f, 5)
    return pg.PoemGenerator(training_texts=[f], markov_length=markov_length).chains.the_mapping


class SimilarityCache(object):
    """This class used to be the object that managed the global cache of text
    similarities. It was a clunky homebrew system and has been superseded by the
    (new) SimilarityCache. It is no longer used, but its code remains in case files
    generated by it ever have to be read.

    The object's internal data cache is a dictionary:
        { (text_name_one, text_name_two):         (a tuple)
              { 'when':,                          (a datetime: when the calculation was made)
                'similarity':                     (a value between 0 and 1, rather heavily weighted toward zero)
                  }
             }
    """
    def __init__(self, cache_file=similarity_cache_location):
        try:
            with bz2.open(similarity_cache_location, "rb") as pickled_file:
                log_it("Loading cached similarity data ...", 3)
                self._data = pickle.load(pickled_file)
        except (OSError, EOFError, AttributeError, pickle.PicklingError) as err:
            log_it("WARNING! Unable to load cached similarity data because %s. Preparing empty cache ..." % err)
            self._data = dict()
        self._dirty = False
        self._cache_file = cache_file

    def __str__(self):
        try:
            return "< Textual Similarity Cache, with %d results cached >" % len(self._data)
        except AttributeError:
            return "< Textual Similarity Cache (not fully initialized: no data attached) >"
        except BaseException as err:
            return "< Textual Similarity Cache (unknown state because %s) >" % err

    def flush_cache(self):
        """Writes the textual similarity cache to disk, if self._dirty is True. If
        self._dirty is False, it silently returns without doing anything.

        Or, rather, that's the basic idea. In fact, what it does is reload the version
        of the cache that's currently on disk and updates it with new info instead of
        replacing the one on disk. The reason for this, of course, is that this
        script has become complex enough that it may take more than an hour to run on
        the slow old laptop that hosts it ... and so there may be multiple copies
        running, each of which thinks it has the "master copy" in memory. To help
        ameliorate the potential for race conditions, we update instead of overwriting.

        #FIXME: This function does not do any file locking; there's nothing preventing
        multiple attempts to update the cache at the same time. The convention for
        reducing this problem is that any code, before calling flush_cache(), must
        acquire a PidFile lock distinct from the one the main script acquires before
        beginning its run. Code that needs to write to the cache needs to repeatedly
        attempt to acquire this lock, waiting in between failed attempts, until it is
        able to do so. See .build_cache() for an example.

        In fact, we should be using some sort of real database-like thing, because the
        overhead of keeping all this data in memory could in theory grow quite large.
        """
        if not self._dirty:
            log_it("Skipping cache update: no changes made!", 4)
            return
        log_it("Updating similarity data cache on disk ...", 3)
        try:
            with bz2.open(self._cache_file, "rb") as pickled_file:
                old = pickle.load(pickled_file)
            old.update(self._data)
            self._data = old
        except (OSError, EOFError, pickle.PicklingError) as err:
            log_it("Not able to update previous data: %s" % err)
        except BaseException as err:
            log_it("Unhandled exception occurred!   %s" % err)
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._data, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
        log_it(" ... updated!", 3)
        self._dirty = False

    @staticmethod
    def calculate_overlap(one, two):
        """return the percentage of chains in dictionary ONE that are also in
        dictionary TWO.
        """
        overlap_count = 0
        for which_chain in one.keys():
            if which_chain in two: overlap_count += 1
        return overlap_count / len(one)

    def calculate_similarity(self, one, two, markov_length=5):
        """Come up with a score evaluating how similar the two texts are to each other.
        This actually means, more specifically, "the product of (a) the percentage of
        chains in the set of chains of length MARKOV_LENGTH constructed from text ONE
        that are also in text TWO; multiplied by (b) the percentage of chains of
        length MARKOV_LENGTH constructed from text TWO that are also in chains
        constructed from text ONE.

        This routine also caches the calculated result in the global similarity cache.
        It's a comparatively expensive calculation to make, so we store the results.
        """
        log_it("calculate_similarity() called for: %s" % [one, two], 5)
        if one == two:
            return 1                        # Well, that's easy.
        chains_one = get_mappings(one, markov_length)
        chains_two = get_mappings(two, markov_length)
        ret = self.calculate_overlap(chains_one, chains_two) * self.calculate_overlap(chains_two, chains_one)
        self._data[tuple(sorted([os.path.basename(one), os.path.basename(two)]))] = {'when': time.time(), 'similarity': ret,}
        self._dirty = True
        return ret

    def get_similarity(self, one, two):
        """Checks to see if the similarity between ONE and TWO is already known. If it is,
        returns that similarity. Otherwise, calculates the similarity and stores it in
        the global similarity cache, which is written at the end of the script's run.

        In short, this function takes advantage of the memoization of
        calculate_similarity, also taking taking advantage of the fact that
        calculate_similarity(A, B) = calculate_similarity(B, A). It also watches to make
        sure that neither of the texts involved has been changed since the calculation
        was initially made. If either has, it re-performs the calculation and stores
        the updated result in the cache.

        Note that calculate_similarity() itself stores the results of the function. This
        function only takes advantage of the stored values.
        """
        # Index in lexicographical order, by actual filename, after dropping path
        index = tuple(sorted([os.path.basename(one), os.path.basename(two)]))
        log_it("get_similarity() called for files: %s" % list(index), 5)

        if index in self._data:                       # If it's in the cache, and the data isn't stale ...
            if self._data[index]['when'] < os.path.getmtime(one):
                log_it("  ... but cached data is stale relative to %s !" % one, 6)
                return self.calculate_similarity(one, two)
            if self._data[index]['when'] < os.path.getmtime(two):
                log_it("  ... but cached data is stale relative to %s !" % two, 6)
                return self.calculate_similarity(one, two)
            log_it(" ... returning cached value!", 6)
            return self._data[index]['similarity']

        log_it(" ... not found in cache! Calculating and cacheing ...", 6)
        return self.calculate_similarity(one, two)

    def build_cache(self):
        """Sequentially go through the corpus, text by text, forcing comparisons to all
        other texts and cacheing the results, to make sure the cache is fully
        populated. Periodically, it dumps the results to disk by updating the on-disk
        cache, so that not all of the calculation results are lost if the run is
        interrupted.

        This method takes a VERY long time to run if starting from an empty cache with
        many source texts in the corpus. The cache CAN OF COURSE be allowed to
        populate itself across multiple runs: this particular method is completely
        unneeded for anything other than some testing applications.
        """
        log_it("Building cache ...")
        for i, first_text in enumerate(sorted(glob.glob(os.path.join(poetry_corpus, '*')))):
            if i % 5 == 0:
                log_it("  We've performed full calculations for %d texts!" % i)
                if i % 10 == 0:
                    while self._dirty:
                        try:
                            with pid.PidFile(piddir=home_dir):
                                self.flush_cache()                          # Note that success clears self._dirty.
                        except pid.PidFileError:
                            time.sleep(5)                                   # In use? Wait and try again.
            for j, second_text in enumerate(sorted(glob.glob(os.path.join(poetry_corpus, '*')))):
                log_it("About to compare %s to %s ..." % (os.path.basename(first_text), os.path.basename(second_text)), 6)
                _ = self.get_similarity(first_text, second_text)

    def clean_cache(self):
        """Run through the cache, checking for problems and fixing them. Work on a copy
        of the data, then rebind the copy over the original data after cleaning is
        done.
        """
        pruned = self._data.copy()
        for count, (one, two) in enumerate(self._data):
            try:
                if count % 1000 == 0:
                    log_it("We're on entry # %d: that's %d %% done!" % (count, (100 * count/len(self._data))))
                assert os.path.isfile(os.path.join(poetry_corpus, one)), "'%s' does not exist!" % one
                assert os.path.isfile(os.path.join(poetry_corpus, two)), "'%s' does not exist!" % two
                assert one <= two, "%s and %s are mis-ordered!" % (one, two)
                assert self._data[(one, two)]['when'] >= os.path.getmtime(os.path.join(poetry_corpus, one)), "data for '%s' is stale!" % one
                assert self._data[(one, two)]['when'] >= os.path.getmtime(os.path.join(poetry_corpus, two)), "data for '%s' is stale!" % two
                _ = int(self._data[(one, two)]['when'])
            except (AssertionError, ValueError, KeyError) as err:
                log_it("Removing entry: (%s, %s)    -- because: %s" % (one, two, err))
                del pruned[(one, two)]
                self._dirty = True
            except BaseException as err:
                log_it("Unhandled error: %s! Leaving data in place" % err)
        removed = len(self._data) - len(pruned)
        log_it("Removed %d entries; that's %d %%!" % (removed, 100 * removed/len(self._data)))
        self._data = pruned
        # We're now going to flush the newly cleaned cache directly to disk. Note that
        # we're not UPDATING THE CACHE using flush_cache(), because that would just
        # allow stale old data that we just cleaned out to propagate back in.
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._data, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
            self._dirty = False
            log_it("Cache updated!")


class NewSimilarityCache(SimilarityCache):
    """This SimilarityCache subclass uses a pair of pandas
    """
    _instance = None

    def __new__(cls, *pargs, **kwargs):
        """Enforce the requirement that this be a singleton class"""
        if not cls._instance:
            cls._instance = SimilarityCache.__new__(cls)
        return cls._instance

    def __init__(self, cache_file=similarity_cache_location):
        """Set up a new instance of this object. There should only ever be one.
        Try to read in cached data if it exists in the expected location. If not,
        create a new blank cache.
        """
        self._dirty = False
        self._cache_file = cache_file
        try:
            with bz2.open(self._cache_file, mode='rb') as pickled_file:
                self._similarity_data = pickle.load(pickled_file)
                self._calculation_times = pickle.load(pickled_file)
        except BaseException as err:
            log_it("WARNING! Unable to decode similarity cache because %s. Creating new from scratch ..." % err)
            self._similarity_data = pd.Series(dict(), dtype="float16")
            self._calculation_times = pd.Series(dict(), dtype="float64")

    def __repr__(self):
        try:
            return "< (new-style) Textual Similarity Cache, with %d results cached >" % self._similarity_data.size
        except AttributeError:
            return "< (new-style) Textual Similarity Cache (not fully initialized: no data attached) >"
        except BaseException as err:
            return "< (new-style) Textual Similarity Cache (unknown state because [ %s ]) >" % err

    def clean_cache(self):
        raise NotImplementedError("#FIXME: cleaning the cache is not yet implemented for the new-style similarity cache!")

    def flush_cache(self):
        """Writes the textual similarity cache to disk, if self._dirty is True. If
        self._dirty is False, it silently returns without doing anything.

        #FIXME: This function does not do any file locking; there's nothing preventing
        multiple attempts to update the cache at the same time. The convention for
        reducing this problem is that any code, before calling flush_cache(), must
        acquire a PidFile lock distinct from the one the main script acquires before
        beginning its run. Code that needs to write to the cache needs to repeatedly
        attempt to acquire this lock, waiting in between failed attempts, until it is
        able to do so. See .build_cache() for an example.
        """
        if not self._dirty:
            log_it("Skipping cache update: no changes made!", 4)
            return
        log_it("Updating similarity data cache on disk ...", 3)
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._similarity_data, pickled_file)
            pickle.dump(self._calculation_times, pickled_file)
        log_it(" ... updated successfully!", 3)
        self._dirty = False

    @staticmethod
    def _key_name_from_text_names(one, two):
        """Takes ONE and TWo (filenames of source texts) and produces a normalized key
        used to index the similarity cache to find or store the similarity between those
        two texts. Returns a string, which is that key.

        Uses two squiggle arrows pointing in opposite directions as a separator, because
        that currently seems to be highly unlikely to occur in the titles of source
        texts.
        """
        one = os.path.basename(one.strip())
        two = os.path.basename(two.strip())
        if one > two:
            one, two = two, one
        return """%s⇜⇝%s""" % (one, two)

    def _store_similarity(self, one, two, similarity):
        """Stores the SIMILARITY (a floating-point number between zero and one, heavily
        weighted toward 0) between ONE and TWO (filenames for texts being compared) in
        the data frames that keep data for this cache. The cache also stores a timestamp
        for when the calculation was performed, which is not passed to this function.
        Instead, the current time is used, on the assumption that the calculation has
        JUST been performed. This could in theory cause unknown stale data in the cache,
        if the source text is updated between when the calculation has been performed
        and the time when the timestamp is finally recorded, but oh well: in this case,
        the gap is generally quite small, in fact, and the implications of stale data
        here are quite small--just that similarity calculations may be using slightly
        incorrect data. Since major updates to source texts are unlikely (we're never
        going to replace the text of 'The Rime of the Ancient Mariner' with the text of
        'The Sick Rose', though we might occasionally correct spelling or punctuation),
        large changes to similarity numbers are also unlikely. We can live with a small,
        unlikely race condition here.

        This function makes no attempt to check whether there is already data stored
        for that key.
        """
        key = self._key_name_from_text_names(one, two)
        self._similarity_data[key] = similarity
        self._calculation_times = time.time()
        self._dirty = True

    def calculate_similarity(self, one, two, markov_length=5):
        log_it("calculate_similarity() called for: %s" % [one, two], 5)
        if one == two:
            return 1                        # Well, that's easy.
        chains_one = get_mappings(one, markov_length)
        chains_two = get_mappings(two, markov_length)
        ret = self.calculate_overlap(chains_one, chains_two) * self.calculate_overlap(chains_two, chains_one)
        self._store_similarity(one, two, ret)
        return ret

    def get_similarity(self, one, two):
        key = self._key_name_from_text_names(one, two)
        if not key in self._similarity_data:
            log_it("  ... not found in cache! Calculating and caching ...", 6)
            return self.calculate_similarity(one, two)
        if self._calculation_times[key] < os.path.getmtime(one):
            log_it("  ... but cached data is stale relative to %s !" % one, 6)
            return self.calculate_similarity(one, two)
        if self._calculation_times[key] < os.path.getmtime(two):
            log_it("  ... but cached data is stale relative to %s !" % two, 6)
            return self.calculate_similarity(one, two)
        log_it("  ... returning cached value!", 6)
        return self._similarity_data[key]


import numpy as np                                      # http://www.numpy.org/

class BadSimilarityCache(SimilarityCache):
    """This SimilarityCache is a singleton object that manages the global cache of the
    results of similarity calculations. Calculating the similarity between two texts
    is a comparatively time- and memory-intensive operation, so we cache the results
    on disk to speed up textual selection.

    Unlike the previous iteration, this is not implemented under the hood as a
    sorted tuple-indexed dictionary yielding another dictionary: this is a pair
    of pandas DataFrames, so the calculations and cacheing should work much more
    quickly than before. Also unlike its superclass, flushing the cache to disk
    overwrites, rather than

    Interesting object attributes:
      * self._dirty             has the data changed since it was last written to
                                disk?
      * self._cache_file        full path to file on disk where this cache is
                                stored.
      * self._similarity_data   table of zero-weighted two-byte 0-to-1 floats
                                indicating calculated similarity scores. Two bytes
                                is good enough for our purposes here.
      * self._calculation_times table of eight-byte floats encoding Unix timestamp
                                for when calculation was performed. (Eight bytes are
                                needed to keep the precision of the Unix timestamp.)

    This SimilarityCache instance is VERY VERY SLOW.
    """
    _instance = None

    def __new__(cls, *args, **kwargs):
        """Enforce the requirement that this be a singleton class"""
        if not cls._instance:
            cls._instance = SimilarityCache.__new__(cls, *args, **kwargs)
        return cls._instance

    def __init__(self, cache_file=similarity_cache_location):
        """Set up a new instance of this object. There should only ever be one.
        Try to read in cached data if it exists in the expected location. If not,
        create a new blank cache.

        #FIXME: we need to validate the correctness of the data after loading and
        discard any rows or columns whose names are stale! We need to create rows
        and columns for any new texts that have appeared!
        """
        self._dirty = False
        self._cache_file = cache_file
        try:
            with bz2.open(self._cache_file, mode='rb') as pickled_file:
                self._similarity_data = pickle.load(pickled_file)
                self._calculation_times = pickle.load(pickled_file)
        except BaseException as err:
            log_it("WARNING! Unable to decode similarity cache because %s. Creating new from scratch ..." % err)
            poem_files = sorted([os.path.basename(f) for f in glob.glob(os.path.join(poetry_corpus, '*'))])
            self._similarity_data = pd.DataFrame(np.full((len(poem_files), len(poem_files)), np.nan, dtype="float16"), index=poem_files, columns=poem_files)
            self._calculation_times = pd.DataFrame(np.full((len(poem_files), len(poem_files)), np.nan, dtype="float64"), index=poem_files, columns=poem_files)

    def __str__(self):
        try:
            return "< (BAD-style) Textual Similarity Cache, with %d results cached >" % sum(self._similarity_data.count())
        except AttributeError:
            return "< (BAD-style) Textual Similarity Cache (not fully initialized: no data attached) >"
        except BaseException as err:
            return "< (BAD-style) Textual Similarity Cache (unknown state because %s) >" % err

    def clean_cache(self):
        raise NotImplementedError("#FIXME: cleaning the cache is not yet implemented for the slow-ass similarity cache!")

    def flush_cache(self):
        """Writes the textual similarity cache to disk, if self._dirty is True. If
        self._dirty is False, it silently returns without doing anything.

        #FIXME: This function does not do any file locking; there's nothing preventing
        multiple attempts to update the cache at the same time. The convention for
        reducing this problem is that any code, before calling flush_cache(), must
        acquire a PidFile lock distinct from the one the main script acquires before
        beginning its run. Code that needs to write to the cache needs to repeatedly
        attempt to acquire this lock, waiting in between failed attempts, until it is
        able to do so. See .build_cache() for an example.
        """
        if not self._dirty:
            log_it("Skipping cache update: no changes made!", 4)
            return
        log_it("Updating similarity data cache on disk ...", 3)
        with bz2.open(self._cache_file, 'wb') as pickled_file:
            pickle.dump(self._similarity_data, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
            pickle.dump(self._calculation_times, pickled_file, protocol=pickle.HIGHEST_PROTOCOL)
        log_it(" ... updated successfully!", 3)
        self._dirty = False

    def calculate_similarity(self, one, two, markov_length=5):
        """Come up with a score evaluating how similar the two texts are to each other.
        This actually means, more specifically, "the product of (a) the percentage of
        chains in the set of chains of length MARKOV_LENGTH constructed from text ONE
        that are also in text TWO; multiplied by (b) the percentage of chains of
        length MARKOV_LENGTH constructed from text TWO that are also in chains
        constructed from text ONE.

        This routine also caches the calculated result in the global similarity cache.
        It's a comparatively expensive calculation to make, so we store the results.
        """
        log_it("calculate_similarity() called for: %s" % [one, two], 5)
        if one == two:
            return 1                        # Well, that's easy.
        elif one > two:
            one, two = two, one
        chains_one = get_mappings(one, markov_length)
        chains_two = get_mappings(two, markov_length)
        ret = self.calculate_overlap(chains_one, chains_two) * self.calculate_overlap(chains_two, chains_one)
        one, two = os.path.basename(one), os.path.basename(two)
        self._similarity_data[one][two] = ret
        self._calculation_times[one][two] = time.time()
        self._dirty = True
        return ret

    def get_similarity(self, one, two):
        """Checks to see if the similarity between ONE and TWO is already known. If it is,
        returns that similarity. Otherwise, calculates the similarity and stores it in
        the global similarity cache, which is written at the end of the script's run.

        In short, this function takes advantage of the memoization of
        calculate_similarity, also taking taking advantage of the fact that
        calculate_similarity(A, B) = calculate_similarity(B, A). It also watches to make
        sure that neither of the texts involved has been changed since the calculation
        was initially made. If either has, it re-performs the calculation and stores
        the updated result in the cache.

        Note that calculate_similarity() itself stores the results of the function. This
        function only takes advantage of the stored values.
        """
        # Index in lexicographical order, by actual filename, after dropping path
        log_it("get_similarity() called for files: %s" % [one, two], 5)
        if one > two:
            one, two = two, one
        if np.isnan(self._calculation_times[os.path.basename(one)][os.path.basename(two)]):
            log_it("  ... not found in cache! Calculating and caching ...", 6)
            return self.calculate_similarity(one, two)
        if self._calculation_times[os.path.basename(one)][os.path.basename(two)] < os.path.getmtime(one):
            log_it("  ... but cached data is stale relative to %s !" % one, 6)
            return self.calculate_similarity(one, two)
        if self._calculation_times[os.path.basename(one)][os.path.basename(two)] < os.path.getmtime(two):
            log_it("  ... but cached data is stale relative to %s !" % two, 6)
            return self.calculate_similarity(one, two)
        log_it("  ... returning cached value!", 6)
        return self._similarity_data[os.path.basename(one)][os.path.basename(two)]



if __name__ == "__main__":
    # Debugging tests
    import random
    c = BadSimilarityCache()
    print(c)
    for i in range(100):
        a, b = random.choice(glob.glob(os.path.join(poetry_corpus, '*'))), random.choice(glob.glob(os.path.join(poetry_corpus, '*')))
        print("Similarity between %s and %s is: %.4f" % (os.path.basename(a), os.path.basename(b), c.get_similarity(a,b)))
